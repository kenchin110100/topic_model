{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\"\"\"\n",
    "ノード分割アルゴリズム部分のコードの書き換え\n",
    "alpha = 全単語数に対する語彙の頻度数, beta = 頻度の割合\n",
    "\"\"\"\n",
    "from igraph import *\n",
    "import csv\n",
    "import collections\n",
    "import pickle\n",
    "import numpy as np\n",
    "from openopt import QP\n",
    "import copy\n",
    "import re\n",
    "\n",
    "# csvファイルの読み込み\n",
    "def readcsv(path):\n",
    "    f = open(path, \"rb\")\n",
    "    dataReader = csv.reader(f)\n",
    "    arr = [row for row in dataReader]\n",
    "    return arr\n",
    "\n",
    "def writecsv(arr, path):\n",
    "    f = open(path, \"ab\")\n",
    "    dataWriter = csv.writer(f)\n",
    "    dataWriter.writerows(arr)\n",
    "    f.close()\n",
    "\n",
    "def readdump(path):\n",
    "    f = open(path, \"r\")\n",
    "    arr = pickle.load(f)\n",
    "    f.close()\n",
    "    return arr\n",
    "\n",
    "# 有向エッジリストを入力して、重み付き無向ネットワークを出力する\n",
    "def cal_edgelist_to_network(list_edge):\n",
    "    # 有向エッジリストを無向エッジリストに変換する\n",
    "    list_edge = [tuple(sorted(row)) for row in list_edge]\n",
    "    # ノードリスト\n",
    "    list_vertices = list(set([word for row in list_edge for word in row]))\n",
    "    # エッジリストとそのweightを作成\n",
    "    tuple_edge, tuple_weight = zip(*collections.Counter(list_edge).items())\n",
    "    return {\"vertex\": list_vertices, \"edge\": list(tuple_edge), \"weight\": list(tuple_weight)}\n",
    "    \n",
    "    \n",
    "# クラスタリング済みのネットワークを元にサブグラフのリストを作成\n",
    "# vertexには全てのvertexを代入する（PageRankを計算するため）\n",
    "def cal_cluster_to_network(dict_network):\n",
    "    if dict_network.has_key(\"cluster\") == False:\n",
    "        print \"クラスタリングができていません\"\n",
    "    \n",
    "    # クラスタごとにwordをまとめる\n",
    "    dict_cluster = collections.defaultdict(list)\n",
    "    for word, cluster in zip(dict_network[\"vertex\"], dict_network[\"cluster\"]):\n",
    "        dict_cluster[cluster].append(word)\n",
    "\n",
    "    # リストに変換\n",
    "    list_cluster_vertex = [row[1] for row in dict_cluster.items()]\n",
    "    \n",
    "    # 同様にエッジとウェイトのリストも作成する\n",
    "    list_cluster_edge = []\n",
    "    list_cluster_weight = []\n",
    "    for cluster_vertex in list_cluster_vertex:\n",
    "        list_cluster_edge_one = []\n",
    "        list_cluster_weight_one = []\n",
    "        # エッジリストの中に、一つでもノードが含まれていれば、そのクラスのノードに含める\n",
    "        for row, weight in zip(dict_network[\"edge\"], dict_network[\"weight\"]):\n",
    "            # and と or を切り替えることによって性能の比較\n",
    "            if row[0] in cluster_vertex or row[1] in cluster_vertex:\n",
    "                list_cluster_edge_one.append(row)\n",
    "                list_cluster_weight_one.append(weight)\n",
    "        list_cluster_edge.append(list_cluster_edge_one)\n",
    "        list_cluster_weight.append(list_cluster_weight_one)\n",
    "    \n",
    "    # まとめる\n",
    "    list_dict_network = [{\"vertex\": dict_network[\"vertex\"],\n",
    "                          \"edge\": cluster_edge,\n",
    "                          \"weight\": cluster_weight}\n",
    "                         for cluster_edge, cluster_weight\n",
    "                         in zip(list_cluster_edge, list_cluster_weight)]\n",
    "    \n",
    "    return list_dict_network\n",
    "\n",
    "# f_measureを計算する\n",
    "def cal_f_measure(list_predict_measure):\n",
    "    # 生成したクラスタ内のカウント\n",
    "    dict_predict_cluster = collections.defaultdict(list)\n",
    "    for row in list_predict_measure:\n",
    "        dict_predict_cluster[row[0]].append(row[1])\n",
    "        \n",
    "    # もとあるクラス内のカウント\n",
    "    dict_measure_cluster = collections.defaultdict(list)\n",
    "    for row in list_predict_measure:\n",
    "        dict_measure_cluster[row[1]].append(row[0])\n",
    "    \n",
    "    # local_purityの計算\n",
    "    list_purity = []\n",
    "    for row in dict_predict_cluster.items():\n",
    "        major_class = sorted(collections.Counter(row[1]).items(), key=lambda x: x[1], reverse=True)[0][1]\n",
    "        class_num = len(row[1])\n",
    "        list_purity.append([major_class, class_num])\n",
    "    purity = float(np.sum(zip(*list_purity)[0])) / np.sum(zip(*list_purity)[1])\n",
    "    print \"Purity: \", purity\n",
    "    \n",
    "    # inverse_purityの計算\n",
    "    list_inverse_purity = []\n",
    "    for row in dict_measure_cluster.items():\n",
    "        major_class = sorted(collections.Counter(row[1]).items(), key=lambda x: x[1], reverse=True)[0][1]\n",
    "        class_num = len(row[1])\n",
    "        list_inverse_purity.append([major_class, class_num])\n",
    "    inverse_purity = float(np.sum(zip(*list_inverse_purity)[0])) / np.sum(zip(*list_inverse_purity)[1])\n",
    "    print \"Inverse Purity: \", inverse_purity\n",
    "    \n",
    "    print \"F-value: \", 2 / (1 / purity + 1 / inverse_purity)\n",
    "    \n",
    "# 凸２次計画問題を解いてp(topic)を求めるための関数\n",
    "def cal_prob_topic(dict_network_master, list_dict_network_sub):\n",
    "    prob_master = np.array([row[1] for row in sorted(zip(dict_network_master[\"vertex\"], dict_network_master[\"page_rank\"]), key=lambda x: x[0])])\n",
    "    \n",
    "    for i, dict_network_sub in enumerate(list_dict_network_sub):\n",
    "        if i == 0:\n",
    "            prob_sub = np.array([row[1] for row in sorted(zip(dict_network_sub[\"vertex\"], dict_network_sub[\"page_rank\"]), key=lambda x: x[0])])\n",
    "        else:\n",
    "            list_tmp = np.array([row[1] for row in sorted(zip(dict_network_sub[\"vertex\"], dict_network_sub[\"page_rank\"]), key=lambda x: x[0])])\n",
    "            prob_sub = np.vstack((prob_sub, list_tmp))\n",
    "    \n",
    "    H = 2 * prob_sub.dot(prob_sub.T)\n",
    "    f = -2 * prob_master.dot(prob_sub.T)\n",
    "    Aeq = np.ones(len(list_dict_network_sub))\n",
    "    beq = 1\n",
    "    lb = np.zeros(len(list_dict_network_sub))\n",
    "    \n",
    "    p = QP(H, f, Aeq=Aeq, beq=beq, lb=lb)\n",
    "    r = p.solve(\"cvxopt_qp\")\n",
    "    k_opt = r.xf\n",
    "    return k_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# クラスタリング後に、ノードごとにどのクラスタにエッジを持っているのか計算\n",
    "def cal_cluster_to_edge(dict_network, low_fleq=0.05, low_rate=0.8, flag=0):\n",
    "    if dict_network.has_key(\"cluster\") == False:\n",
    "        print \"クラスタリングができていません\"\n",
    "    \n",
    "    # wordをclusterに変換するための辞書を作成する\n",
    "    dict_word_to_cluster = {}\n",
    "    for word, cluster in zip(dict_network[\"vertex\"], dict_network[\"cluster\"]):\n",
    "        dict_word_to_cluster[word] = cluster\n",
    "        \n",
    "    # wordをidに変換するための辞書を作成する\n",
    "    dict_word_to_id = {}\n",
    "    for i, word in enumerate(dict_network[\"vertex\"]):\n",
    "        dict_word_to_id[word] = i\n",
    "    \n",
    "    # clusterの数\n",
    "    num_cluster = len(set(dict_network[\"cluster\"]))\n",
    "    \n",
    "    # id_to_clusterのマトリックス\n",
    "    matrix_id_to_cluster = np.zeros((len(dict_word_to_id), num_cluster))\n",
    "    for row, weight in zip(dict_network[\"edge\"], dict_network[\"weight\"]):\n",
    "        matrix_id_to_cluster[dict_word_to_id[row[0]]][dict_word_to_cluster[row[1]]] += weight\n",
    "        matrix_id_to_cluster[dict_word_to_id[row[1]]][dict_word_to_cluster[row[0]]] += weight\n",
    "        \n",
    "    #総単語数を求める\n",
    "    total_voc = np.sum(matrix_id_to_cluster)\n",
    "    \n",
    "    # 指定したハイパーパラメータよりも高い値を記録した単語を所属クラスタとともに辞書に登録する\n",
    "    dict_word_to_list_cluster = {}\n",
    "    for cluster, word, row in zip(dict_network[\"cluster\"], dict_network[\"vertex\"], matrix_id_to_cluster):\n",
    "        top_num = max(row)\n",
    "        if float(np.sum(row))/total_voc>=low_fleq and len(np.where(row/top_num>=low_rate)[0])>=2:\n",
    "            print word\n",
    "            dict_word_to_list_cluster[word] = np.where(row/top_num>=low_rate)[0]\n",
    "            \n",
    "    # 分割する単語が存在するかしないかflagを立てる\n",
    "    if len(dict_word_to_list_cluster) > 0:\n",
    "        outflag = True\n",
    "    else:\n",
    "        outflag = False\n",
    "\n",
    "    # 新しく分裂するノードクラスターを元のクラスターに記録\n",
    "    for word, row in dict_word_to_list_cluster.items():\n",
    "        for num in row:\n",
    "            # dict_word_to_cluster[word+\"_\"+str(num)] = num\n",
    "            dict_word_to_cluster[word+\"_\"+str(num)] = dict_word_to_cluster[word]\n",
    "    \n",
    "    # ここの計算が間違ってそう => diffsplitで確認したが大丈夫そう・・・\n",
    "    for word in dict_word_to_list_cluster.keys():\n",
    "        list_edge_new = []\n",
    "        list_weight_new = []\n",
    "        for row, weight in zip(dict_network[\"edge\"], dict_network[\"weight\"]):\n",
    "            if row[0] == word:\n",
    "                if dict_word_to_cluster[row[1]] in dict_word_to_list_cluster[word]:\n",
    "                    list_edge_new.append([row[0]+\"_\"+str(dict_word_to_cluster[row[1]]), row[1]])\n",
    "                    list_weight_new.append(weight)\n",
    "                else:\n",
    "                    for num in dict_word_to_list_cluster[word]:\n",
    "                        # flagの値によって、weightの計算法を切り替え\n",
    "                        if flag == 0:\n",
    "                            weight_tmp = float(weight)/len(dict_word_to_list_cluster[word])\n",
    "                            list_edge_new.append([row[0]+\"_\"+str(num), row[1]])\n",
    "                            list_weight_new.append(weight_tmp)\n",
    "                        else:\n",
    "                            weight_tmp = weight/len(dict_word_to_list_cluster[word])\n",
    "                            if weight_tmp > 0:\n",
    "                                list_edge_new.append([row[0]+\"_\"+str(num), row[1]])\n",
    "                                list_weight_new.append(weight_tmp)\n",
    "                                \n",
    "            elif row[1] == word:\n",
    "                if dict_word_to_cluster[row[0]] in dict_word_to_list_cluster[word]:\n",
    "                    list_edge_new.append([row[0], row[1]+\"_\"+str(dict_word_to_cluster[row[0]])])\n",
    "                    list_weight_new.append(weight)\n",
    "                else:\n",
    "                    for num in dict_word_to_list_cluster[word]:\n",
    "                        # flagの値によって、weightの計算法を切り替え\n",
    "                        if flag == 0:\n",
    "                            weight_tmp = float(weight)/len(dict_word_to_list_cluster[word])\n",
    "                            list_edge_new.append([row[0], row[1]+\"_\"+str(num)])\n",
    "                            list_weight_new.append(weight_tmp)\n",
    "                        else:\n",
    "                            weight_tmp = weight/len(dict_word_to_list_cluster[word])\n",
    "                            if weight_tmp > 0:\n",
    "                                list_edge_new.append([row[0], row[1]+\"_\"+str(num)])\n",
    "                                list_weight_new.append(weight_tmp)\n",
    "            else:\n",
    "                list_edge_new.append([row[0], row[1]])\n",
    "                list_weight_new.append(weight)\n",
    "        else:\n",
    "            dict_network[\"edge\"] = copy.deepcopy(list_edge_new)\n",
    "            dict_network[\"weight\"] = copy.deepcopy(list_weight_new)\n",
    "            list_vertices = list(set([word for row in dict_network[\"edge\"] for word in row]))\n",
    "            dict_network[\"vertex\"] = list_vertices\n",
    "    \n",
    "    return dict_network, outflag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###メイン部分\n",
    "1. 有向エッジリストから無向重み付きエッジリストを作成\n",
    "2. louvain法によるクラスタリング\n",
    "3. クラスタごとにpagerankを計算し、p(word|topic)とする\n",
    "4. クラスタごとにp(word|topic)を出力する辞書を作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "温泉\n",
      "プラン\n",
      "機会\n",
      "あと\n",
      "なかった\n",
      "ビジネスホテル\n",
      "値段\n",
      "いい\n",
      "十分\n",
      "得\n",
      "他\n",
      "仕事\n",
      "よい\n",
      "充実\n",
      "欲しい\n",
      "必要\n",
      "内容\n",
      "大変\n",
      "家族\n",
      "シャワー\n",
      "きれい\n",
      "お世話になりました\n",
      "お願い\n",
      "気持ちよい\n",
      "満足\n",
      "宿\n",
      "用意\n",
      "残念\n",
      "掃除\n",
      "無い\n",
      "1 回目\n",
      "10914\n",
      "10914\n",
      "ない\n",
      "2 回目\n",
      "11124\n",
      "11124\n",
      "3 回目\n",
      "11124\n",
      "11124\n",
      "クラスタ数:  6\n",
      "\n",
      "------------------------- OpenOpt 0.5625 -------------------------\n",
      "problem: unnamed   type: QP\n",
      "solver: cvxopt_qp\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -2.5309e-03 -1.0031e+00  1e+00  2e-16  3e+00\n",
      " 1: -2.5313e-03 -1.3055e-02  1e-02  1e-16  3e-02\n",
      " 2: -2.5580e-03 -3.0008e-03  4e-04  7e-17  1e-03\n",
      " 3: -2.6018e-03 -2.6361e-03  3e-05  2e-16  2e-05\n",
      " 4: -2.6033e-03 -2.6041e-03  8e-07  2e-16  2e-07\n",
      " 5: -2.6033e-03 -2.6033e-03  8e-09  9e-17  2e-09\n",
      "Optimal solution found.\n",
      "istop: 1000 (optimal)\n",
      "Solver:   Time Elapsed = 0.0 \tCPU Time Elapsed = 0.0\n",
      "objFuncValue: -0.0026032717 (feasible, MaxResidual = 0)\n"
     ]
    }
   ],
   "source": [
    "# エッジリストの読み込み\n",
    "list_edge = readcsv(\"./files/rakuten_corpus/rakuten_corpus_edgelist.csv\")\n",
    "# 元のネットワークを作成する（無向）\n",
    "dict_network_master = cal_edgelist_to_network(list_edge)\n",
    "# g = Graph(directed=True)\n",
    "g_master = Graph()\n",
    "g_master.add_vertices(dict_network_master[\"vertex\"])\n",
    "g_master.add_edges(dict_network_master[\"edge\"])\n",
    "# 元のネットワークのpagerankを求める\n",
    "dict_network_master[\"page_rank\"] = g_master.pagerank(directed=False, weights=dict_network_master[\"weight\"])\n",
    "\n",
    "flag=True\n",
    "counter=0\n",
    "while flag:\n",
    "    # louvain法によるクラスタリング、vertexと同じ長さのクラスタ番号が書かれたリストがreturn\n",
    "    dict_network_master[\"cluster\"] = g_master.community_fastgreedy(weights=dict_network_master[\"weight\"]).as_clustering(n=6).membership\n",
    "    # クラスタリングの結果をもとに、どのノードから何本のエッジが出ているか計算する\n",
    "    dict_network_master, flag = cal_cluster_to_edge(dict_network_master, low_fleq=0.002, low_rate=0.6, flag=0)\n",
    "    counter +=1\n",
    "    print counter, \"回目\"\n",
    "    print len(dict_network_master[\"weight\"])\n",
    "    print len(dict_network_master[\"edge\"])\n",
    "    g_master = Graph()\n",
    "    g_master.add_vertices(dict_network_master[\"vertex\"])\n",
    "    g_master.add_edges(dict_network_master[\"edge\"])\n",
    "\n",
    "dict_network_master[\"cluster\"] = g_master.community_fastgreedy(weights=dict_network_master[\"weight\"]).as_clustering(n=6).membership\n",
    "# 元のネットワークのpagerankを求める\n",
    "dict_network_master[\"page_rank\"] = g_master.pagerank(directed=False, weights=dict_network_master[\"weight\"])\n",
    "# クラスタ結果をもとにサブグラフのリストを作成\n",
    "list_dict_network_sub = cal_cluster_to_network(dict_network_master)\n",
    "# サブクラスタごとに中心性の計算\n",
    "for i, dict_network_sub in enumerate(list_dict_network_sub):\n",
    "    g_sub = Graph()\n",
    "    g_sub.add_vertices(dict_network_sub[\"vertex\"])\n",
    "    g_sub.add_edges(dict_network_sub[\"edge\"])\n",
    "    list_dict_network_sub[i][\"page_rank\"] = g_sub.pagerank(directed=False, weights=dict_network_sub[\"weight\"])\n",
    "print \"クラスタ数: \", len(list_dict_network_sub)\n",
    "\n",
    "# トピックごとにwordを入力したらp(word|topic)が出るような辞書を作成\n",
    "list_dict_prob = []\n",
    "for i in range(len(list_dict_network_sub)):\n",
    "    list_word_page = sorted(zip(list_dict_network_sub[i][\"vertex\"], list_dict_network_sub[i][\"page_rank\"]), key=lambda x: x[1], reverse=True)\n",
    "    list_word_page_rev = []\n",
    "    for row in list_word_page:\n",
    "        pattern = \"_[0-9]+\"\n",
    "        word = re.sub(pattern, \"\", row[0])\n",
    "        list_word_page_rev.append([word, row[1]]) \n",
    "    list_dict_prob.append({row[0]: row[1] for row in list_word_page_rev})\n",
    "    \n",
    "# 凸２次計画問題を解いて、p(topic)を求める\n",
    "list_prob_topic = cal_prob_topic(dict_network_master, list_dict_network_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "予約 0.0336970436092\n",
      "出張 0.0208632894504\n",
      "確認 0.00932700105509\n",
      "ツイン 0.009225442074\n",
      "是非 0.00846394105412\n",
      "連絡 0.00842891556384\n",
      "泊 0.00830725519894\n",
      "変更 0.00806830354263\n",
      "改善 0.00784465830907\n",
      "お世話 0.00783298015713\n"
     ]
    }
   ],
   "source": [
    "num = 4\n",
    "list_tmp = sorted(list_dict_prob[num].items(), key=lambda x: x[1], reverse=True)\n",
    "for i in range(10):\n",
    "    print list_tmp[i][0], list_tmp[i][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定量評価部分\n",
    "1. 形態素解析済みセンテンスを読み込み、確率を計算し、どのクラスに属するか計算\n",
    "2. 予測クラスと実際クラスのリストを作成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "エラーデータ数:  2\n"
     ]
    }
   ],
   "source": [
    "list_dict_words = dict_network_master[\"vertex\"]\n",
    "list_sep_words = readdump(\"./files/rakuten_corpus/annotation/all_sep.dump\")\n",
    "list_sep_words_rev = []\n",
    "for row in list_sep_words:\n",
    "    list_sep_words_rev.append([row[2], row[1]])\n",
    "# 確率が最大になるクラスを予想する\n",
    "# 実質ラベルがintじゃない場合は、エラーとして、記録しない\n",
    "list_predict_measure = []\n",
    "list_words_rev = []\n",
    "error_count = 0\n",
    "for row in list_sep_words_rev:\n",
    "    try:\n",
    "        class_tmp = 0\n",
    "        prob_tmp = 0\n",
    "        predict_class = []\n",
    "        for num, dict_prob in enumerate(list_dict_prob):\n",
    "            prob_tmp_tmp = 1\n",
    "            for word in row[0]:\n",
    "                prob_tmp_tmp *= float(dict_prob[word])\n",
    "            prob_tmp_tmp *= list_prob_topic[num]\n",
    "            if prob_tmp_tmp > prob_tmp:\n",
    "                class_tmp = num\n",
    "                prob_tmp = prob_tmp_tmp\n",
    "        list_predict_measure.append([class_tmp, row[1]])\n",
    "    except:\n",
    "        error_count += 1\n",
    "print \"エラーデータ数: \", error_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###計算結果の表示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 0.01, 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity:  0.592592592593\n",
      "Inverse Purity:  0.651651651652\n",
      "F-value:  0.620720479128\n"
     ]
    }
   ],
   "source": [
    "cal_f_measure(list_predict_measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 0.001, 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity:  0.593093093093\n",
      "Inverse Purity:  0.670670670671\n",
      "F-value:  0.629500787917\n"
     ]
    }
   ],
   "source": [
    "cal_f_measure(list_predict_measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 0.002, 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity:  0.595595595596\n",
      "Inverse Purity:  0.679179179179\n",
      "F-value:  0.634647211012\n"
     ]
    }
   ],
   "source": [
    "cal_f_measure(list_predict_measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 0.003, 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity:  0.601601601602\n",
      "Inverse Purity:  0.672172172172\n",
      "F-value:  0.63493198503\n"
     ]
    }
   ],
   "source": [
    "cal_f_measure(list_predict_measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 0.004, 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity:  0.592592592593\n",
      "Inverse Purity:  0.666166166166\n",
      "F-value:  0.627229217289\n"
     ]
    }
   ],
   "source": [
    "cal_f_measure(list_predict_measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 0.005, 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity:  0.594094094094\n",
      "Inverse Purity:  0.666166166166\n",
      "F-value:  0.628069292485\n"
     ]
    }
   ],
   "source": [
    "cal_f_measure(list_predict_measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity:  0.589089089089\n",
      "Inverse Purity:  0.657157157157\n",
      "F-value:  0.621264236124\n"
     ]
    }
   ],
   "source": [
    "cal_f_measure(list_predict_measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 100, 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity:  0.592592592593\n",
      "Inverse Purity:  0.666166166166\n",
      "F-value:  0.627229217289\n"
     ]
    }
   ],
   "source": [
    "cal_f_measure(list_predict_measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 50, 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity:  0.602102102102\n",
      "Inverse Purity:  0.67017017017\n",
      "F-value:  0.634315275149\n"
     ]
    }
   ],
   "source": [
    "cal_f_measure(list_predict_measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 100, 0.7, or、切り捨てで計算した場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity:  0.567091087169\n",
      "Inverse Purity:  0.629285014691\n",
      "F-value:  0.596571467059\n"
     ]
    }
   ],
   "source": [
    "cal_f_measure(list_predict_measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 100, 0.9, or, 切り捨てで計算した場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity:  0.487093835408\n",
      "Inverse Purity:  0.623443668387\n",
      "F-value:  0.546898356081\n"
     ]
    }
   ],
   "source": [
    "cal_f_measure(list_predict_measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 50, 0.8, or, 切り捨てで計算した結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity:  0.494078348011\n",
      "Inverse Purity:  0.638931065897\n",
      "F-value:  0.557245159054\n"
     ]
    }
   ],
   "source": [
    "cal_f_measure(list_predict_measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 50, 0.7, or, 切り捨てで計算した結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity:  0.486486486486\n",
      "Inverse Purity:  0.649559672032\n",
      "F-value:  0.556318949262\n"
     ]
    }
   ],
   "source": [
    "cal_f_measure(list_predict_measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity:  0.492256301245\n",
      "Inverse Purity:  0.639234740358\n",
      "F-value:  0.556199417134\n"
     ]
    }
   ],
   "source": [
    "cal_f_measure(list_predict_measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
