{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\"\"\"\n",
    "ノード分割アルゴリズム部分のコードの書き換え\n",
    "alpha = 全単語数に対する語彙の頻度数, beta = 頻度の割合\n",
    "\"\"\"\n",
    "from igraph import *\n",
    "import csv\n",
    "import collections\n",
    "import pickle\n",
    "import numpy as np\n",
    "from openopt import QP\n",
    "import copy\n",
    "import re\n",
    "\n",
    "# csvファイルの読み込み\n",
    "def readcsv(path):\n",
    "    f = open(path, \"rb\")\n",
    "    dataReader = csv.reader(f)\n",
    "    arr = [row for row in dataReader]\n",
    "    return arr\n",
    "\n",
    "def writecsv(arr, path):\n",
    "    f = open(path, \"ab\")\n",
    "    dataWriter = csv.writer(f)\n",
    "    dataWriter.writerows(arr)\n",
    "    f.close()\n",
    "\n",
    "def readdump(path):\n",
    "    f = open(path, \"r\")\n",
    "    arr = pickle.load(f)\n",
    "    f.close()\n",
    "    return arr\n",
    "\n",
    "# 有向エッジリストを入力して、重み付き無向ネットワークを出力する\n",
    "def cal_edgelist_to_network(list_edge):\n",
    "    # 有向エッジリストを無向エッジリストに変換する\n",
    "    list_edge = [tuple(sorted(row)) for row in list_edge]\n",
    "    # ノードリスト\n",
    "    list_vertices = list(set([word for row in list_edge for word in row]))\n",
    "    # エッジリストとそのweightを作成\n",
    "    tuple_edge, tuple_weight = zip(*collections.Counter(list_edge).items())\n",
    "    return {\"vertex\": list_vertices, \"edge\": list(tuple_edge), \"weight\": list(tuple_weight)}\n",
    "    \n",
    "    \n",
    "# クラスタリング済みのネットワークを元にサブグラフのリストを作成\n",
    "# vertexには全てのvertexを代入する（PageRankを計算するため）\n",
    "def cal_cluster_to_network(dict_network):\n",
    "    if dict_network.has_key(\"cluster\") == False:\n",
    "        print \"クラスタリングができていません\"\n",
    "    \n",
    "    # クラスタごとにwordをまとめる\n",
    "    dict_cluster = collections.defaultdict(list)\n",
    "    for word, cluster in zip(dict_network[\"vertex\"], dict_network[\"cluster\"]):\n",
    "        dict_cluster[cluster].append(word)\n",
    "\n",
    "    # リストに変換\n",
    "    list_cluster_vertex = [row[1] for row in dict_cluster.items()]\n",
    "    \n",
    "    # 同様にエッジとウェイトのリストも作成する\n",
    "    list_cluster_edge = []\n",
    "    list_cluster_weight = []\n",
    "    for cluster_vertex in list_cluster_vertex:\n",
    "        list_cluster_edge_one = []\n",
    "        list_cluster_weight_one = []\n",
    "        # エッジリストの中に、一つでもノードが含まれていれば、そのクラスのノードに含める\n",
    "        for row, weight in zip(dict_network[\"edge\"], dict_network[\"weight\"]):\n",
    "            # and と or を切り替えることによって性能の比較\n",
    "            if row[0] in cluster_vertex or row[1] in cluster_vertex:\n",
    "                list_cluster_edge_one.append(row)\n",
    "                list_cluster_weight_one.append(weight)\n",
    "        list_cluster_edge.append(list_cluster_edge_one)\n",
    "        list_cluster_weight.append(list_cluster_weight_one)\n",
    "    \n",
    "    # まとめる\n",
    "    list_dict_network = [{\"vertex\": dict_network[\"vertex\"],\n",
    "                          \"edge\": cluster_edge,\n",
    "                          \"weight\": cluster_weight}\n",
    "                         for cluster_edge, cluster_weight\n",
    "                         in zip(list_cluster_edge, list_cluster_weight)]\n",
    "    \n",
    "    return list_dict_network\n",
    "\n",
    "# f_measureを計算する\n",
    "def cal_f_measure(list_predict_measure):\n",
    "    # 生成したクラスタ内のカウント\n",
    "    dict_predict_cluster = collections.defaultdict(list)\n",
    "    for row in list_predict_measure:\n",
    "        dict_predict_cluster[row[0]].append(row[1])\n",
    "        \n",
    "    # もとあるクラス内のカウント\n",
    "    dict_measure_cluster = collections.defaultdict(list)\n",
    "    for row in list_predict_measure:\n",
    "        dict_measure_cluster[row[1]].append(row[0])\n",
    "    \n",
    "    # local_purityの計算\n",
    "    list_purity = []\n",
    "    for row in dict_predict_cluster.items():\n",
    "        major_class = sorted(collections.Counter(row[1]).items(), key=lambda x: x[1], reverse=True)[0][1]\n",
    "        class_num = len(row[1])\n",
    "        list_purity.append([major_class, class_num])\n",
    "    purity = float(np.sum(zip(*list_purity)[0])) / np.sum(zip(*list_purity)[1])\n",
    "    print \"Purity: \", purity\n",
    "    \n",
    "    # inverse_purityの計算\n",
    "    list_inverse_purity = []\n",
    "    for row in dict_measure_cluster.items():\n",
    "        major_class = sorted(collections.Counter(row[1]).items(), key=lambda x: x[1], reverse=True)[0][1]\n",
    "        class_num = len(row[1])\n",
    "        list_inverse_purity.append([major_class, class_num])\n",
    "    inverse_purity = float(np.sum(zip(*list_inverse_purity)[0])) / np.sum(zip(*list_inverse_purity)[1])\n",
    "    print \"Inverse Purity: \", inverse_purity\n",
    "    \n",
    "    print \"F-value: \", 2 / (1 / purity + 1 / inverse_purity)\n",
    "    \n",
    "# 凸２次計画問題を解いてp(topic)を求めるための関数\n",
    "def cal_prob_topic(dict_network_master, list_dict_network_sub):\n",
    "    prob_master = np.array([row[1] for row in sorted(zip(dict_network_master[\"vertex\"], dict_network_master[\"page_rank\"]), key=lambda x: x[0])])\n",
    "    \n",
    "    for i, dict_network_sub in enumerate(list_dict_network_sub):\n",
    "        if i == 0:\n",
    "            prob_sub = np.array([row[1] for row in sorted(zip(dict_network_sub[\"vertex\"], dict_network_sub[\"page_rank\"]), key=lambda x: x[0])])\n",
    "        else:\n",
    "            list_tmp = np.array([row[1] for row in sorted(zip(dict_network_sub[\"vertex\"], dict_network_sub[\"page_rank\"]), key=lambda x: x[0])])\n",
    "            prob_sub = np.vstack((prob_sub, list_tmp))\n",
    "    \n",
    "    H = 2 * prob_sub.dot(prob_sub.T)\n",
    "    f = -2 * prob_master.dot(prob_sub.T)\n",
    "    Aeq = np.ones(len(list_dict_network_sub))\n",
    "    beq = 1\n",
    "    lb = np.zeros(len(list_dict_network_sub))\n",
    "    \n",
    "    p = QP(H, f, Aeq=Aeq, beq=beq, lb=lb)\n",
    "    r = p.solve(\"cvxopt_qp\")\n",
    "    k_opt = r.xf\n",
    "    return k_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# クラスタリング後に、ノードごとにどのクラスタにエッジを持っているのか計算\n",
    "def cal_cluster_to_edge(dict_network, low_fleq=0.05, low_rate=0.8, flag=0):\n",
    "    if dict_network.has_key(\"cluster\") == False:\n",
    "        print \"クラスタリングができていません\"\n",
    "    \n",
    "    # wordをclusterに変換するための辞書を作成する\n",
    "    dict_word_to_cluster = {}\n",
    "    for word, cluster in zip(dict_network[\"vertex\"], dict_network[\"cluster\"]):\n",
    "        dict_word_to_cluster[word] = cluster\n",
    "        \n",
    "    # wordをidに変換するための辞書を作成する\n",
    "    dict_word_to_id = {}\n",
    "    for i, word in enumerate(dict_network[\"vertex\"]):\n",
    "        dict_word_to_id[word] = i\n",
    "    \n",
    "    # clusterの数\n",
    "    num_cluster = len(set(dict_network[\"cluster\"]))\n",
    "    \n",
    "    # id_to_clusterのマトリックス\n",
    "    matrix_id_to_cluster = np.zeros((len(dict_word_to_id), num_cluster))\n",
    "    for row, weight in zip(dict_network[\"edge\"], dict_network[\"weight\"]):\n",
    "        matrix_id_to_cluster[dict_word_to_id[row[0]]][dict_word_to_cluster[row[1]]] += weight\n",
    "        matrix_id_to_cluster[dict_word_to_id[row[1]]][dict_word_to_cluster[row[0]]] += weight\n",
    "        \n",
    "    #総単語数を求める\n",
    "    total_voc = np.sum(matrix_id_to_cluster)\n",
    "    \n",
    "    # 指定したハイパーパラメータよりも高い値を記録した単語を所属クラスタとともに辞書に登録する\n",
    "    dict_word_to_list_cluster = {}\n",
    "    for cluster, word, row in zip(dict_network[\"cluster\"], dict_network[\"vertex\"], matrix_id_to_cluster):\n",
    "        top_num = max(row)\n",
    "        if float(np.sum(row))/total_voc>=low_fleq and len(np.where(row/top_num>=low_rate)[0])>=2:\n",
    "            print word\n",
    "            dict_word_to_list_cluster[word] = np.where(row/top_num>=low_rate)[0]\n",
    "            \n",
    "    # 分割する単語が存在するかしないかflagを立てる\n",
    "    if len(dict_word_to_list_cluster) > 0:\n",
    "        outflag = True\n",
    "    else:\n",
    "        outflag = False\n",
    "\n",
    "    # 新しく分裂するノードクラスターを元のクラスターに記録\n",
    "    for word, row in dict_word_to_list_cluster.items():\n",
    "        for num in row:\n",
    "            # dict_word_to_cluster[word+\"_\"+str(num)] = num\n",
    "            dict_word_to_cluster[word+\"_\"+str(num)] = dict_word_to_cluster[word]\n",
    "    \n",
    "    # ここの計算が間違ってそう => diffsplitで確認したが大丈夫そう・・・\n",
    "    for word in dict_word_to_list_cluster.keys():\n",
    "        list_edge_new = []\n",
    "        list_weight_new = []\n",
    "        for row, weight in zip(dict_network[\"edge\"], dict_network[\"weight\"]):\n",
    "            if row[0] == word:\n",
    "                if dict_word_to_cluster[row[1]] in dict_word_to_list_cluster[word]:\n",
    "                    list_edge_new.append([row[0]+\"_\"+str(dict_word_to_cluster[row[1]]), row[1]])\n",
    "                    list_weight_new.append(weight)\n",
    "                else:\n",
    "                    for num in dict_word_to_list_cluster[word]:\n",
    "                        # flagの値によって、weightの計算法を切り替え\n",
    "                        if flag == 0:\n",
    "                            weight_tmp = float(weight)/len(dict_word_to_list_cluster[word])\n",
    "                            list_edge_new.append([row[0]+\"_\"+str(num), row[1]])\n",
    "                            list_weight_new.append(weight_tmp)\n",
    "                        else:\n",
    "                            weight_tmp = weight/len(dict_word_to_list_cluster[word])\n",
    "                            if weight_tmp > 0:\n",
    "                                list_edge_new.append([row[0]+\"_\"+str(num), row[1]])\n",
    "                                list_weight_new.append(weight_tmp)\n",
    "                                \n",
    "            elif row[1] == word:\n",
    "                if dict_word_to_cluster[row[0]] in dict_word_to_list_cluster[word]:\n",
    "                    list_edge_new.append([row[0], row[1]+\"_\"+str(dict_word_to_cluster[row[0]])])\n",
    "                    list_weight_new.append(weight)\n",
    "                else:\n",
    "                    for num in dict_word_to_list_cluster[word]:\n",
    "                        # flagの値によって、weightの計算法を切り替え\n",
    "                        if flag == 0:\n",
    "                            weight_tmp = float(weight)/len(dict_word_to_list_cluster[word])\n",
    "                            list_edge_new.append([row[0], row[1]+\"_\"+str(num)])\n",
    "                            list_weight_new.append(weight_tmp)\n",
    "                        else:\n",
    "                            weight_tmp = weight/len(dict_word_to_list_cluster[word])\n",
    "                            if weight_tmp > 0:\n",
    "                                list_edge_new.append([row[0], row[1]+\"_\"+str(num)])\n",
    "                                list_weight_new.append(weight_tmp)\n",
    "            else:\n",
    "                list_edge_new.append([row[0], row[1]])\n",
    "                list_weight_new.append(weight)\n",
    "        else:\n",
    "            dict_network[\"edge\"] = copy.deepcopy(list_edge_new)\n",
    "            dict_network[\"weight\"] = copy.deepcopy(list_weight_new)\n",
    "            list_vertices = list(set([word for row in dict_network[\"edge\"] for word in row]))\n",
    "            dict_network[\"vertex\"] = list_vertices\n",
    "    \n",
    "    return dict_network, outflag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###メイン部分\n",
    "1. 有向エッジリストから無向重み付きエッジリストを作成\n",
    "2. louvain法によるクラスタリング\n",
    "3. クラスタごとにpagerankを計算し、p(word|topic)とする\n",
    "4. クラスタごとにp(word|topic)を出力する辞書を作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "普通\n",
      "温泉\n",
      "条件\n",
      "プラン\n",
      "人\n",
      "声\n",
      "壁\n",
      "駐車場\n",
      "大浴場\n",
      "なかった\n",
      "ビジネスホテル\n",
      "非常\n",
      "静か\n",
      "子供\n",
      "いい\n",
      "悪い\n",
      "評価\n",
      "安い\n",
      "料金\n",
      "よい\n",
      "最高\n",
      "充実\n",
      "寒い\n",
      "遅い\n",
      "ない\n",
      "大変\n",
      "車\n",
      "すごい\n",
      "お願い\n",
      "外\n",
      "気持ちよい\n",
      "用意\n",
      "気\n",
      "残念\n",
      "掃除\n",
      "無い\n",
      "感じ\n",
      "アメニティ\n",
      "1 回目\n",
      "24556\n",
      "24556\n",
      "他\n",
      "2 回目\n",
      "24667\n",
      "24667\n",
      "3 回目\n",
      "24667\n",
      "24667\n",
      "クラスタ数:  9\n",
      "\n",
      "------------------------- OpenOpt 0.5625 -------------------------\n",
      "problem: unnamed   type: QP\n",
      "solver: cvxopt_qp\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -2.1325e-03 -1.0039e+00  1e+00  1e-16  3e+00\n",
      " 1: -2.1342e-03 -1.3903e-02  1e-02  1e-16  4e-02\n",
      " 2: -2.2418e-03 -3.6634e-03  1e-03  1e-16  5e-03\n",
      " 3: -2.4544e-03 -2.6648e-03  2e-04  5e-17  2e-18\n",
      " 4: -2.4755e-03 -2.4889e-03  1e-05  5e-17  1e-18\n",
      " 5: -2.4764e-03 -2.4769e-03  5e-07  7e-17  1e-18\n",
      " 6: -2.4764e-03 -2.4764e-03  6e-09  6e-17  2e-18\n",
      "Optimal solution found.\n",
      "istop: 1000 (optimal)\n",
      "Solver:   Time Elapsed = 0.0 \tCPU Time Elapsed = 0.0\n",
      "objFuncValue: -0.0024763653 (feasible, MaxResidual = 1.11022e-16)\n"
     ]
    }
   ],
   "source": [
    "# エッジリストの読み込み\n",
    "list_edge = readcsv(\"./files/rakuten_corpus/rakuten_corpus_edgelist_co.csv\")\n",
    "# 元のネットワークを作成する（無向）\n",
    "dict_network_master = cal_edgelist_to_network(list_edge)\n",
    "# g = Graph(directed=True)\n",
    "g_master = Graph()\n",
    "g_master.add_vertices(dict_network_master[\"vertex\"])\n",
    "g_master.add_edges(dict_network_master[\"edge\"])\n",
    "# 元のネットワークのpagerankを求める\n",
    "dict_network_master[\"page_rank\"] = g_master.pagerank(directed=False, weights=dict_network_master[\"weight\"])\n",
    "\n",
    "flag=True\n",
    "counter=0\n",
    "while flag:\n",
    "    # louvain法によるクラスタリング、vertexと同じ長さのクラスタ番号が書かれたリストがreturn\n",
    "    dict_network_master[\"cluster\"] = g_master.community_multilevel(weights=dict_network_master[\"weight\"]).membership\n",
    "    # クラスタリングの結果をもとに、どのノードから何本のエッジが出ているか計算する\n",
    "    dict_network_master, flag = cal_cluster_to_edge(dict_network_master, low_fleq=0.002, low_rate=0.6, flag=0)\n",
    "    counter +=1\n",
    "    print counter, \"回目\"\n",
    "    print len(dict_network_master[\"weight\"])\n",
    "    print len(dict_network_master[\"edge\"])\n",
    "    g_master = Graph()\n",
    "    g_master.add_vertices(dict_network_master[\"vertex\"])\n",
    "    g_master.add_edges(dict_network_master[\"edge\"])\n",
    "\n",
    "#dict_network_master[\"cluster\"] = g_master.community_multilevel(weights=dict_network_master[\"weight\"], return_levels=True)[-3].membership\n",
    "dict_network_master[\"cluster\"] = g_master.community_multilevel(weights=dict_network_master[\"weight\"]).membership\n",
    "# 元のネットワークのpagerankを求める\n",
    "dict_network_master[\"page_rank\"] = g_master.pagerank(directed=False, weights=dict_network_master[\"weight\"])\n",
    "# クラスタ結果をもとにサブグラフのリストを作成\n",
    "list_dict_network_sub = cal_cluster_to_network(dict_network_master)\n",
    "# サブクラスタごとに中心性の計算\n",
    "for i, dict_network_sub in enumerate(list_dict_network_sub):\n",
    "    g_sub = Graph()\n",
    "    g_sub.add_vertices(dict_network_sub[\"vertex\"])\n",
    "    g_sub.add_edges(dict_network_sub[\"edge\"])\n",
    "    list_dict_network_sub[i][\"page_rank\"] = g_sub.pagerank(directed=False, weights=dict_network_sub[\"weight\"])\n",
    "print \"クラスタ数: \", len(list_dict_network_sub)\n",
    "\n",
    "# トピックごとにwordを入力したらp(word|topic)が出るような辞書を作成\n",
    "list_dict_prob = []\n",
    "for i in range(len(list_dict_network_sub)):\n",
    "    list_word_page = sorted(zip(list_dict_network_sub[i][\"vertex\"], list_dict_network_sub[i][\"page_rank\"]), key=lambda x: x[1], reverse=True)\n",
    "    list_word_page_rev = []\n",
    "    for row in list_word_page:\n",
    "        pattern = \"_[0-9]+\"\n",
    "        word = re.sub(pattern, \"\", row[0])\n",
    "        list_word_page_rev.append([word, row[1]]) \n",
    "    list_dict_prob.append({row[0]: row[1] for row in list_word_page_rev})\n",
    "    \n",
    "# 凸２次計画問題を解いて、p(topic)を求める\n",
    "list_prob_topic = cal_prob_topic(dict_network_master, list_dict_network_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "対応 0.0609159023211\n",
      "フロント 0.0438759476186\n",
      "スタッフ 0.0239971555915\n",
      "チェックイン 0.023986567224\n",
      "女性 0.0152126618896\n",
      "従業員 0.0138442335601\n",
      "親切 0.0130850301864\n",
      "丁寧 0.0124230421189\n",
      "チェックアウト 0.0108361451539\n",
      "気持ち 0.00779861781644\n"
     ]
    }
   ],
   "source": [
    "num = 8\n",
    "list_tmp =sorted(list_dict_prob[num].items(), key=lambda x: x[1], reverse=True)\n",
    "for i in range(10):\n",
    "    print list_tmp[i][0], list_tmp[i][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_all = []\n",
    "for num in range(0,31):\n",
    "    list_tmp =sorted(list_dict_prob[num].items(), key=lambda x: x[1], reverse=True)\n",
    "    for i in range(10):\n",
    "        list_all.append([list_tmp[i][0], list_tmp[i][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writecsv(list_all, \"./files/hieral_co.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_0 = [0]\n",
    "class_1 = [3,9,12]\n",
    "class_2 = [8,10,18]\n",
    "class_3 = [11,17,20,23]\n",
    "class_4 = [1,2,5,21,22]\n",
    "class_5 = [4,16,24,26]\n",
    "class_6 = [6,7,13,14,27,30]\n",
    "class_7 = [15,19,28]\n",
    "class_8 = [25,29]\n",
    "dict_network_sub0 = []\n",
    "dict_network_sub1 = []\n",
    "dict_network_sub2 = []\n",
    "dict_network_sub3 = []\n",
    "dict_network_sub4 = []\n",
    "dict_network_sub5 = []\n",
    "dict_network_sub6 = []\n",
    "dict_network_sub7 = []\n",
    "dict_network_sub8 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for num in class_8:\n",
    "    dict_network_sub8.append(list_dict_network[num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_dict_network = copy.deepcopy(list_dict_network_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_dict_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------- OpenOpt 0.5625 -------------------------\n",
      "problem: unnamed   type: QP\n",
      "solver: cvxopt_qp\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -6.9351e-03 -1.0128e+00  1e+00  0e+00  2e+00\n",
      " 1: -7.0022e-03 -2.2697e-02  2e-02  2e-16  3e-02\n",
      " 2: -8.8333e-03 -1.0907e-02  2e-03  3e-16  3e-03\n",
      " 3: -9.6498e-03 -9.9899e-03  3e-04  1e-16  2e-18\n",
      " 4: -9.7638e-03 -9.8080e-03  4e-05  1e-17  3e-18\n",
      " 5: -9.7782e-03 -9.7843e-03  6e-06  2e-16  4e-19\n",
      " 6: -9.7795e-03 -9.7801e-03  6e-07  4e-17  4e-19\n",
      " 7: -9.7795e-03 -9.7795e-03  1e-08  1e-16  2e-18\n",
      " 8: -9.7795e-03 -9.7795e-03  1e-10  1e-16  2e-18\n",
      "Optimal solution found.\n",
      "istop: 1000 (optimal)\n",
      "Solver:   Time Elapsed = 0.0 \tCPU Time Elapsed = 0.0\n",
      "objFuncValue: -0.009779532 (feasible, MaxResidual = 0)\n",
      "[ 0.98800562  0.01199438]\n"
     ]
    }
   ],
   "source": [
    "list_prob_topic = cal_prob_topic(list_dict_network[8], dict_network_sub8)\n",
    "print list_prob_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_network_master[\"cluster\"] = g_master.community_multilevel(weights=dict_network_master[\"weight\"], return_levels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_0 = dict_network_master[\"cluster\"][-3].membership\n",
    "list_1 = dict_network_master[\"cluster\"][-1].membership\n",
    "list_all = []\n",
    "for num0, num1 in zip(list_0, list_1):\n",
    "    list_all.append((num0, num1))\n",
    "list_all = list(set(list_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writecsv(list_all, \"hieral_num_co.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_tmp = g_master.community_multilevel(weights=dict_network_master[\"weight\"], return_levels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_tmp[-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "writecsv(list_all, \"./files/hieral_num.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定量評価部分\n",
    "1. 形態素解析済みセンテンスを読み込み、確率を計算し、どのクラスに属するか計算\n",
    "2. 予測クラスと実際クラスのリストを作成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "エラーデータ数:  2\n"
     ]
    }
   ],
   "source": [
    "list_dict_words = dict_network_master[\"vertex\"]\n",
    "list_sep_words = readdump(\"./files/rakuten_corpus/annotation/all_sep.dump\")\n",
    "list_sep_words_rev = []\n",
    "for row in list_sep_words:\n",
    "    list_sep_words_rev.append([row[2], row[1]])\n",
    "# 確率が最大になるクラスを予想する\n",
    "# 実質ラベルがintじゃない場合は、エラーとして、記録しない\n",
    "list_predict_measure = []\n",
    "list_words_rev = []\n",
    "error_count = 0\n",
    "for row in list_sep_words_rev:\n",
    "    try:\n",
    "        class_tmp = 0\n",
    "        prob_tmp = 0\n",
    "        predict_class = []\n",
    "        for num, dict_prob in enumerate(list_dict_prob):\n",
    "            prob_tmp_tmp = 1\n",
    "            for word in row[0]:\n",
    "                prob_tmp_tmp *= float(dict_prob[word])\n",
    "            prob_tmp_tmp *= list_prob_topic[num]\n",
    "            if prob_tmp_tmp > prob_tmp:\n",
    "                class_tmp = num\n",
    "                prob_tmp = prob_tmp_tmp\n",
    "        list_predict_measure.append([class_tmp, row[1]])\n",
    "    except:\n",
    "        error_count += 1\n",
    "print \"エラーデータ数: \", error_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###計算結果の表示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Louvain法+divide+p(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity:  0.625125125125\n",
      "Inverse Purity:  0.694694694695\n",
      "F-value:  0.658076354701\n"
     ]
    }
   ],
   "source": [
    "cal_f_measure(list_predict_measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Louvain法+fullコーパス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity:  0.584231145935\n",
      "Inverse Purity:  0.652301665034\n",
      "F-value:  0.616392781296\n"
     ]
    }
   ],
   "source": [
    "cal_f_measure(list_predict_measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Louvain法のみ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity:  0.606606606607\n",
      "Inverse Purity:  0.61011011011\n",
      "F-value:  0.608353314236\n"
     ]
    }
   ],
   "source": [
    "cal_f_measure(list_predict_measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Louvain法(一本コーパス)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity:  0.617617617618\n",
      "Inverse Purity:  0.657157157157\n",
      "F-value:  0.636774190759\n"
     ]
    }
   ],
   "source": [
    "cal_f_measure(list_predict_measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 0.004, 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity:  0.592592592593\n",
      "Inverse Purity:  0.666166166166\n",
      "F-value:  0.627229217289\n"
     ]
    }
   ],
   "source": [
    "cal_f_measure(list_predict_measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 0.005, 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity:  0.594094094094\n",
      "Inverse Purity:  0.666166166166\n",
      "F-value:  0.628069292485\n"
     ]
    }
   ],
   "source": [
    "cal_f_measure(list_predict_measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity:  0.589089089089\n",
      "Inverse Purity:  0.657157157157\n",
      "F-value:  0.621264236124\n"
     ]
    }
   ],
   "source": [
    "cal_f_measure(list_predict_measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 100, 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity:  0.592592592593\n",
      "Inverse Purity:  0.666166166166\n",
      "F-value:  0.627229217289\n"
     ]
    }
   ],
   "source": [
    "cal_f_measure(list_predict_measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 50, 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity:  0.602102102102\n",
      "Inverse Purity:  0.67017017017\n",
      "F-value:  0.634315275149\n"
     ]
    }
   ],
   "source": [
    "cal_f_measure(list_predict_measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 100, 0.7, or、切り捨てで計算した場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity:  0.567091087169\n",
      "Inverse Purity:  0.629285014691\n",
      "F-value:  0.596571467059\n"
     ]
    }
   ],
   "source": [
    "cal_f_measure(list_predict_measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 100, 0.9, or, 切り捨てで計算した場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity:  0.487093835408\n",
      "Inverse Purity:  0.623443668387\n",
      "F-value:  0.546898356081\n"
     ]
    }
   ],
   "source": [
    "cal_f_measure(list_predict_measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 50, 0.8, or, 切り捨てで計算した結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity:  0.494078348011\n",
      "Inverse Purity:  0.638931065897\n",
      "F-value:  0.557245159054\n"
     ]
    }
   ],
   "source": [
    "cal_f_measure(list_predict_measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 50, 0.7, or, 切り捨てで計算した結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity:  0.486486486486\n",
      "Inverse Purity:  0.649559672032\n",
      "F-value:  0.556318949262\n"
     ]
    }
   ],
   "source": [
    "cal_f_measure(list_predict_measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity:  0.492256301245\n",
      "Inverse Purity:  0.639234740358\n",
      "F-value:  0.556199417134\n"
     ]
    }
   ],
   "source": [
    "cal_f_measure(list_predict_measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list1 = g_master.community_fastgreedy(weights=dict_network_master[\"weight\"]).as_clustering(n=6).subgraphs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list2 = g_master.community_fastgreedy(weights=dict_network_master[\"weight\"]).as_clustering(n=7).subgraphs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    list1[1].vertex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Graph' object has no attribute 'vertex'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-643d9a7bc97f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlist1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvertex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Graph' object has no attribute 'vertex'"
     ]
    }
   ],
   "source": [
    "list1[1].vertex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
