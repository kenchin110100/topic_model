{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\"\"\"\n",
    "頻度により除去語を指定しないコーパスを使う\n",
    "1文ずつ形態素解析したものと、口コミごとに形態素解析したものの2種類を作成\n",
    "有向グラフ用のエッジリストも作成\n",
    "\"\"\"\n",
    "import MeCab\n",
    "import csv\n",
    "import collections\n",
    "import pickle\n",
    "import itertools\n",
    "from library.filer import Filer\n",
    "import glob\n",
    "import random\n",
    "import re\n",
    "    \n",
    "def parsing(sentence):\n",
    "    mecab = MeCab.Tagger('-d /usr/local/lib/mecab/dic/mecab-ipadic-neologd')\n",
    "    res = mecab.parseToNode(sentence)\n",
    "    list_words = []\n",
    "    while res:\n",
    "        features = res.feature.split(\",\")\n",
    "        if (features[0] == \"名詞\" and features[1] in [\"一般\", \"固有名詞\", \"サ変接続\", \"形容動詞語幹\"]) or features[0] == \"形容詞\":\n",
    "            if features[6] == \"*\":\n",
    "                if res.surface != '':\n",
    "                    list_words.append(res.surface)\n",
    "            else:\n",
    "                if features[6] != '':\n",
    "                    list_words.append(features[6])\n",
    "        res = res.next\n",
    "    return list_words\n",
    "\n",
    "def parsing_rev(sentence):\n",
    "    mecab = MeCab.Tagger('-d /usr/local/lib/mecab/dic/mecab-ipadic-neologd')\n",
    "    res = mecab.parseToNode(sentence)\n",
    "    list_words = []\n",
    "    while res:\n",
    "        features = res.feature.split(\",\")\n",
    "        if features[0] != \"記号\":\n",
    "            if features[6] == \"*\":\n",
    "                if res.surface != '':\n",
    "                    list_words.append(res.surface)\n",
    "            else:\n",
    "                if features[6] != '':\n",
    "                    list_words.append(features[6])\n",
    "        res = res.next\n",
    "    return list_words\n",
    "\n",
    "# csvファイルの読み込み\n",
    "def readcsv(path):\n",
    "    f = open(path, \"rb\")\n",
    "    dataReader = csv.reader(f)\n",
    "    arr = [row for row in dataReader]\n",
    "    return arr\n",
    "\n",
    "def readtsv(path):\n",
    "    f = open(path, \"rb\")\n",
    "    dataReader = csv.reader(f, delimiter='\\t')\n",
    "    arr = [row for row in dataReader]\n",
    "    return arr\n",
    "\n",
    "def writecsv(arr, path):\n",
    "    f = open(path, \"ab\")\n",
    "    dataWriter = csv.writer(f)\n",
    "    dataWriter.writerows(arr)\n",
    "    f.close()\n",
    "\n",
    "def writedump(arr, path):\n",
    "    f = open(path, \"w\")\n",
    "    pickle.dump(arr, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### コーパス作成\n",
    "1. 同じユーザーの口コミを1文にまとめる\n",
    "2. ユーザーごとに形態素解析したリストを作成\n",
    "3. 高頻度、低頻度語を削除するために、除去語リストを作成する\n",
    "4. 除去"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_sentences = readtsv(\"./files/rakuten_corpus/annotation01_tsukuba_corpus_20140930.tsv\")\n",
    "# 同じユーザーの口コミを1文にまとめる\n",
    "list_sentences_rev = []\n",
    "sentence_tmp = \"\"\n",
    "for row in list_sentences:\n",
    "    if row[5] != \"\":\n",
    "        sentence_tmp += row[5]\n",
    "    else:\n",
    "        list_sentences_rev.append(sentence_tmp)\n",
    "        sentence_tmp = \"\"\n",
    "list_sentences_rev.append(sentence_tmp)\n",
    "\n",
    "\n",
    "# 形態素解析したリストを作成\n",
    "list_words = [parsing(row) for row in list_sentences_rev]\n",
    "\n",
    "\n",
    "\n",
    "# 各単語の頻度を数え上げる\n",
    "list_words_collection = []\n",
    "for row in list_words:\n",
    "    list_words_collection.extend(row)\n",
    "\n",
    "# 除去語を省く前の語彙数と語数\n",
    "print len(list_words_collection)\n",
    "print len(set(list_words_collection))\n",
    "    \n",
    "list_words_collection = collections.Counter(list_words_collection).items()\n",
    "\n",
    "# 除去語リストを作成\n",
    "list_words_remove = []\n",
    "for row in list_words_collection:\n",
    "    if row[1] < 3 or row[1] > 300:\n",
    "        list_words_remove.append(row[0])\n",
    "\n",
    "# 除去語リスト内の単語を削除\n",
    "list_words_rev = []\n",
    "for row in list_words:\n",
    "    list_words_tmp = []\n",
    "    for word in row:\n",
    "        if word in list_words_remove:\n",
    "            pass\n",
    "        else:\n",
    "            list_words_tmp.append(word)\n",
    "    list_words_rev.append(list_words_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### コーパスから有向エッジリストを作成し、保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# エッジリストを作成\n",
    "list_edgelist = []\n",
    "for row in list_words:\n",
    "    for j in range(len(row) - 1):\n",
    "        list_edgelist.append([row[j], row[j+1]])\n",
    "\n",
    "# 作成したエッジリストを保存\n",
    "writecsv(list_edgelist, \"./files/rakuten_corpus_edgelist_full.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UM用のコーパスを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_sentences = readtsv(\"./files/rakuten_corpus/annotation01_tsukuba_corpus_20140930.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 空白行を削除\n",
    "list_sentences_rev = []\n",
    "for row in list_sentences:\n",
    "    if row[5] != '':\n",
    "        list_sentences_rev.append(row[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_words = [parsing(row) for row in list_sentences_rev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writecsv(list_words, \"./files/rakuten_corpus/rakuten_corpus_full_for_UM.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 共起グラフでのエッジリスト作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_sentences = readtsv(\"./files/rakuten_corpus/annotation01_tsukuba_corpus_20140930.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 空白行を削除\n",
    "list_sentences_rev = []\n",
    "for row in list_sentences:\n",
    "    if row[5] != '':\n",
    "        list_sentences_rev.append(row[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_words = [parsing(row) for row in list_sentences_rev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 各単語の頻度を数え上げる\n",
    "list_words_collection = []\n",
    "for row in list_words:\n",
    "    list_words_collection.extend(row)\n",
    "list_words_collection = collections.Counter(list_words_collection).items()\n",
    "\n",
    "# 除去語リストを作成\n",
    "list_words_remove = []\n",
    "for row in list_words_collection:\n",
    "    if row[1] < 3 or row[1] > 300:\n",
    "        list_words_remove.append(row[0])\n",
    "\n",
    "# 除去語リスト内の単語を削除\n",
    "list_words_rev = []\n",
    "for row in list_words:\n",
    "    list_words_tmp = []\n",
    "    for word in row:\n",
    "        if word in list_words_remove:\n",
    "            pass\n",
    "        else:\n",
    "            list_words_tmp.append(word)\n",
    "    list_words_rev.append(list_words_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 共起エッジリストの作成\n",
    "list_edges = []\n",
    "for row1 in list_words_rev:\n",
    "    list_tmp = list(itertools.combinations(row1,2))\n",
    "    for row2 in list_tmp:\n",
    "        list_edges.append(list(row2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writecsv(list_edges, \"./files/rakuten_corpus_edgelist_co.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 共起グラフ+windowでのエッジリスト作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# windowサイズ\n",
    "window = 1\n",
    "list_sentences = readtsv(\"./files/rakuten_corpus/annotation01_tsukuba_corpus_20140930.tsv\")\n",
    "# list_list_bgの形のデータを作る\n",
    "list_sentences_rev = []\n",
    "list_tmp = []\n",
    "for row in list_sentences:\n",
    "    if row[5] != '':\n",
    "        list_tmp.append(row[5])\n",
    "    else:\n",
    "        list_sentences_rev.append(list_tmp)\n",
    "        list_tmp = []\n",
    "else:\n",
    "    list_sentences_rev.append(list_tmp)\n",
    "\n",
    "list_words = [[parsing(sentence) for sentence in row] for row in list_sentences_rev]\n",
    "\n",
    "# 各単語の頻度を数え上げる\n",
    "list_words_collection = []\n",
    "for row in list_words:\n",
    "    for sentence in row:\n",
    "        list_words_collection.extend(sentence)\n",
    "list_words_collection = collections.Counter(list_words_collection).items()\n",
    "\n",
    "# 除去語リストを作成\n",
    "list_words_remove = []\n",
    "for row in list_words_collection:\n",
    "    if row[1] < 3 or row[1] > 300:\n",
    "        list_words_remove.append(row[0])\n",
    "\n",
    "# 除去語リスト内の単語を削除\n",
    "list_words_rev = []\n",
    "for row in list_words:\n",
    "    list_words_tmp1 = []\n",
    "    for sentence in row:\n",
    "        list_words_tmp2 = []\n",
    "        for word in sentence:\n",
    "            if word in list_words_remove:\n",
    "                pass\n",
    "            else:\n",
    "                list_words_tmp2.append(word)\n",
    "        list_words_tmp1.append(list_words_tmp2)\n",
    "    list_words_rev.append(list_words_tmp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_words_rev1 = [[num for word in row for num in word]for row in list_words_rev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "window = 3\n",
    "list_edges = []\n",
    "for row in list_words_rev1:\n",
    "    for i in range(len(row) - window):\n",
    "        list_tmp = list(itertools.combinations(row[i:i+window],2))\n",
    "        for row1 in list_tmp:\n",
    "            list_edges.append(row1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "writecsv(list_edges, \"./files/rakuten_corpus/rakuten_corpus_edgelist_window3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_words_rev2 = []\n",
    "# windowのサイズに従ったbgを作り直す\n",
    "for row in list_words_rev:\n",
    "    len_num = len(row)\n",
    "    for i, sentence in enumerate(row):\n",
    "        list_tmp = []\n",
    "        for num in range(i-window,i+window+1):\n",
    "            if num < 0 or num >= len_num:\n",
    "                continue\n",
    "            else:\n",
    "                list_tmp.extend(row[num])\n",
    "        list_words_rev2.append(list_tmp)\n",
    "        \n",
    "# 共起エッジリストの作成\n",
    "list_edges = []\n",
    "for row1 in list_words_rev2:\n",
    "    list_tmp = list(itertools.combinations(row1,2))\n",
    "    for row2 in list_tmp:\n",
    "        list_edges.append(list(row2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "writecsv(list_edges, \"./files/rakuten_corpus/rakuten_corpus_edgelist_window1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 共起グラフ+windowでのエッジリスト作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# windowサイズ\n",
    "window = 1\n",
    "# weight\n",
    "weight = 0.2\n",
    "list_sentences = readtsv(\"./files/rakuten_corpus/annotation01_tsukuba_corpus_20140930.tsv\")\n",
    "# list_list_bgの形のデータを作る\n",
    "list_sentences_rev = []\n",
    "list_tmp = []\n",
    "for row in list_sentences:\n",
    "    if row[5] != '':\n",
    "        list_tmp.append(row[5])\n",
    "    else:\n",
    "        list_sentences_rev.append(list_tmp)\n",
    "        list_tmp = []\n",
    "else:\n",
    "    list_sentences_rev.append(list_tmp)\n",
    "\n",
    "list_words = [[parsing(sentence) for sentence in row] for row in list_sentences_rev]\n",
    "\n",
    "# 各単語の頻度を数え上げる\n",
    "list_words_collection = []\n",
    "for row in list_words:\n",
    "    for sentence in row:\n",
    "        list_words_collection.extend(sentence)\n",
    "list_words_collection = collections.Counter(list_words_collection).items()\n",
    "\n",
    "# 除去語リストを作成\n",
    "list_words_remove = []\n",
    "for row in list_words_collection:\n",
    "    if row[1] < 3 or row[1] > 300:\n",
    "        list_words_remove.append(row[0])\n",
    "\n",
    "# 除去語リスト内の単語を削除\n",
    "list_words_rev = []\n",
    "for row in list_words:\n",
    "    list_words_tmp1 = []\n",
    "    for sentence in row:\n",
    "        list_words_tmp2 = []\n",
    "        for word in sentence:\n",
    "            if word in list_words_remove:\n",
    "                pass\n",
    "            else:\n",
    "                list_words_tmp2.append(word)\n",
    "        list_words_tmp1.append(list_words_tmp2)\n",
    "    list_words_rev.append(list_words_tmp1)\n",
    "\n",
    "list_edges = []\n",
    "# windowのサイズに従ったbgを作り直す\n",
    "for row in list_words_rev:\n",
    "    len_num = len(row)\n",
    "    for i, sentence in enumerate(row):\n",
    "        list_tmp = []\n",
    "        for num in range(i-window,i+window+1):\n",
    "            if num < 0 or num >= len_num:\n",
    "                continue\n",
    "            else:\n",
    "                if num == i:\n",
    "                    list_tmp = list(itertools.combinations(row[num],2))\n",
    "                    list_edges.extend([[tmp[0], tmp[1], 1] for tmp in list_tmp])\n",
    "                else:\n",
    "                    list_tmp = list(itertools.product(row[num], row[i]))\n",
    "                    list_edges.extend([[tmp[0], tmp[1], weight] for tmp in list_tmp])\n",
    "\n",
    "dict_edges = {}\n",
    "for row in list_edges:\n",
    "    tuple_tmp = tuple(sorted([row[0], row[1]]))\n",
    "    if tuple_tmp in dict_edges:\n",
    "        dict_edges[tuple_tmp] += row[2]\n",
    "    else:\n",
    "        dict_edges[tuple_tmp] = row[2]\n",
    "\n",
    "list_master = [[key[0], key[1], value] for key, value in dict_edges.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "writecsv(list_master, \"./files/rakuten_corpus/rakuten_corpus_edgelist_window_1_0.2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 楽天トラベルのテキストファイルを読み込んで、コーパスサイズごとに分ける"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_master = []\n",
    "list_filename = glob.glob(\"./files/rakuten_corpus/rakuten_corpus_master/txtfile/*.txt\")\n",
    "for path in list_filename:\n",
    "    list_master.extend(Filer.readtxt(path))\n",
    "    \n",
    "list_master_rev = [row.split('\\t')[2] for row in list_master]\n",
    "list_master_rev2 = [row.split('【ご利用の宿泊プラン】')[0] for row in list_master_rev]\n",
    "\n",
    "list_num = [i* 1000 for i in range(1,10)]\n",
    "list_num.extend([i* 10000 for i in range(1,10)])\n",
    "list_num.extend([i* 100000 for i in range(1,10)])\n",
    "list_num.extend([i* 1000000 for i in range(1,4)])\n",
    "\n",
    "list_review = []\n",
    "for num in list_num:\n",
    "    list_review.append(random.sample(list_master_rev2, num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for row in list_review:\n",
    "    Filer.writetxt(row, 'files/rakuten_corpus/rakuten_corpus_master/sentencefile/rakuten_review_%s.txt' % len(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ファイルごとに形態素解析（removeなし）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_master = {}\n",
    "list_filename = glob.glob(\"./files/rakuten_corpus/rakuten_corpus_master/sentencefile/*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for path in list_filename:\n",
    "    list_tmp = Filer.readtxt(path)\n",
    "    dict_master[len(list_tmp)] = list_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_num = [i* 1000 for i in range(1,10)]\n",
    "list_num.extend([i* 10000 for i in range(1,10)])\n",
    "list_num.extend([i* 100000 for i in range(1,10)])\n",
    "list_num.extend([i* 1000000 for i in range(1,4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pattern = re.compile(r'。|！|？|!|\\?')\n",
    "dict_master_rev = {}\n",
    "for num in list_num:\n",
    "    dict_master_rev[num] = []\n",
    "    for user in dict_master[num]:\n",
    "        for sentence in re.split(pattern, user):\n",
    "            if len(sentence.decode('utf-8')) > 1 and sentence != '　' and sentence != ' ':\n",
    "                dict_master_rev[num].append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for num in list_num:\n",
    "    Filer.writetxt(dict_master_rev[num], 'files/rakuten_corpus/rakuten_corpus_master/sentencefile/rakuten_sentence_%s.txt' % num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dict_words = {}\n",
    "for num in list_num:\n",
    "    dict_words[num] = []\n",
    "    for sentence in dict_master_rev[num]:\n",
    "        list_tmp = parsing(sentence)\n",
    "        if len(list_tmp) != 0:\n",
    "            dict_words[num].append(\" \".join(list_tmp))\n",
    "    Filer.writetxt(dict_words[num], \"files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_%s.txt\" % num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 学習コーパス（type1）を作る\n",
    "* 頻度２以下の単語を削除\n",
    "* 単語数が１以下の文を削除\n",
    "* ただしユーザ数が100000以下のファイルのみ（計算が終わらないから）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_filename = [\"./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_1000.txt\",\n",
    "                 \"./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_2000.txt\",\n",
    "                 \"./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_3000.txt\",\n",
    "                 \"./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_4000.txt\",\n",
    "                 \"./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_5000.txt\",\n",
    "                 \"./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_6000.txt\",\n",
    "                 \"./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_7000.txt\",\n",
    "                 \"./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_8000.txt\",\n",
    "                 \"./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_9000.txt\",\n",
    "                 \"./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_10000.txt\",\n",
    "                 \"./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_20000.txt\",\n",
    "                 \"./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_30000.txt\",\n",
    "                 \"./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_40000.txt\",\n",
    "                 \"./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_50000.txt\",\n",
    "                 \"./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_60000.txt\",\n",
    "                 \"./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_70000.txt\",\n",
    "                 \"./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_80000.txt\",\n",
    "                 \"./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_90000.txt\",\n",
    "                 \"./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_100000.txt\"]\n",
    "list_testfile = Filer.readdump('files/rakuten_corpus/annotation/all_sep_full.dump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_1000.txt\n",
      "./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_2000.txt\n",
      "./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_3000.txt\n",
      "./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_4000.txt\n",
      "./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_5000.txt\n",
      "./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_6000.txt\n",
      "./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_7000.txt\n",
      "./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_8000.txt\n",
      "./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_9000.txt\n",
      "./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_10000.txt\n",
      "./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_20000.txt\n",
      "./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_30000.txt\n",
      "./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_40000.txt\n",
      "./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_50000.txt\n",
      "./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_60000.txt\n",
      "./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_70000.txt\n",
      "./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_80000.txt\n",
      "./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_90000.txt\n",
      "./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_100000.txt\n"
     ]
    }
   ],
   "source": [
    "for path in list_filename:\n",
    "    dict_word_freq = {}\n",
    "    list_remove = []\n",
    "    print path\n",
    "    list_word = Filer.readtxt(path)\n",
    "    list_word = [row.split(' ') for row in list_word]\n",
    "    # testファイルの形態素を合体\n",
    "    for row in list_testfile:\n",
    "        list_word.append(row[2])\n",
    "    for row in list_word:\n",
    "        for word in row:\n",
    "            if word in dict_word_freq:\n",
    "                dict_word_freq[word] += 1\n",
    "            else:\n",
    "                dict_word_freq[word] = 1\n",
    "    for word, freq in dict_word_freq.items():\n",
    "        if freq <= 2:\n",
    "            list_remove.append(word)\n",
    "    \n",
    "    list_word = [[word for word in row if word not in list_remove] for row in list_word]\n",
    "    list_word = [\" \".join(row) for row in list_word if len(row) >= 2]\n",
    "    Filer.writetxt(list_word, 'files/rakuten_corpus/rakuten_corpus_master/preprocessedfile/type1/forUM/preprocessed_%s.txt' % len(list_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習コーパス（type1）のPRTM用を作る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_filepath = glob.glob('./files/rakuten_corpus/rakuten_corpus_master/preprocessedfile/type1/forUM/*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for path in list_filepath:\n",
    "    list_sepword = Filer.readtxt(path)\n",
    "    list_sepword = [row.split(' ') for row in list_sepword]\n",
    "    list_edgelist = []\n",
    "    for row in list_sepword:\n",
    "        list_tmp = list(itertools.combinations(row,2))\n",
    "        list_edgelist.extend(list_tmp)\n",
    "    list_edgelist = [' '.join(row) for row in list_edgelist]\n",
    "    path_rev = path.replace('./files/rakuten_corpus/rakuten_corpus_master/preprocessedfile/type1/forUM/preprocessed', '')\n",
    "    Filer.writetxt(list_edgelist, './files/rakuten_corpus/rakuten_corpus_master/preprocessedfile/type1/forPRTM/rakuten_edgelist' + path_rev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習コーパス（type2）を作る\n",
    "* 頻度２以下の単語を削除\n",
    "* 出現頻度6%以上の単語を削除\n",
    "* 単語数が１以下の文を削除\n",
    "* ただしユーザ数が100000以下のファイルのみ（計算が終わらないから）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_filename = [\"./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_1000.txt\",\n",
    "                 \"./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_2000.txt\",\n",
    "                 \"./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_3000.txt\",\n",
    "                 \"./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_4000.txt\",\n",
    "                 \"./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_5000.txt\",\n",
    "                 \"./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_6000.txt\",\n",
    "                 \"./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_7000.txt\",\n",
    "                 \"./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_8000.txt\",\n",
    "                 \"./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_9000.txt\",\n",
    "                 \"./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_10000.txt\",\n",
    "                 \"./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_20000.txt\",\n",
    "                 \"./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_30000.txt\",\n",
    "                 \"./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_40000.txt\",\n",
    "                 \"./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_50000.txt\",\n",
    "                 \"./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_60000.txt\",\n",
    "                 \"./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_70000.txt\",\n",
    "                 \"./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_80000.txt\",\n",
    "                 \"./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_90000.txt\",\n",
    "                 \"./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_100000.txt\"]\n",
    "list_testfile = Filer.readdump('files/rakuten_corpus/annotation/all_sep_full.dump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_1000.txt\n",
      "./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_2000.txt\n",
      "./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_3000.txt\n",
      "./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_4000.txt\n",
      "./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_5000.txt\n",
      "./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_6000.txt\n",
      "./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_7000.txt\n",
      "./files/rakuten_corpus/rakuten_corpus_master/sepwordfile/rakuten_sep_8000.txt"
     ]
    }
   ],
   "source": [
    "for path in list_filename:\n",
    "    dict_word_freq = {}\n",
    "    dict_document_word = {}\n",
    "    list_remove = []\n",
    "    print path\n",
    "    list_word = Filer.readtxt(path)\n",
    "    list_word = [row.split(' ') for row in list_word]\n",
    "    # testファイルの形態素を合体\n",
    "    for row in list_testfile:\n",
    "        list_word.append(row[2])\n",
    "    for row in list_word:\n",
    "        for word in row:\n",
    "            if word in dict_word_freq:\n",
    "                dict_word_freq[word] += 1\n",
    "            else:\n",
    "                dict_word_freq[word] = 1\n",
    "        for word in set(row):\n",
    "            if word in dict_document_word:\n",
    "                dict_document_word[word] += 1\n",
    "            else:\n",
    "                dict_document_word[word] = 1\n",
    "    dict_document_word = {word: float(freq)/len(list_word) for word, freq in dict_document_word.items()}\n",
    "\n",
    "    for word, freq in dict_word_freq.items():\n",
    "        if freq <= 2:\n",
    "            list_remove.append(word)\n",
    "    for word, freq in dict_document_word.items():\n",
    "        if freq >= 0.06:\n",
    "            list_remove.append(word)\n",
    "    \n",
    "    list_word = [[word for word in row if word not in list_remove] for row in list_word]\n",
    "    list_word = [\" \".join(row) for row in list_word if len(row) >= 2]\n",
    "    Filer.writetxt(list_word, 'files/rakuten_corpus/rakuten_corpus_master/preprocessedfile/type2/forUM/rakuten_preprocessed_%s.txt' % len(list_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習コーパス（type2）のPRTM用を作る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_filepath = glob.glob('./files/rakuten_corpus/rakuten_corpus_master/preprocessedfile/type2/forUM/*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./files/rakuten_corpus/rakuten_corpus_master/preprocessedfile/type2/forUM/rakuten_preprocessed_34992.txt',\n",
       " './files/rakuten_corpus/rakuten_corpus_master/preprocessedfile/type2/forUM/rakuten_preprocessed_336795.txt',\n",
       " './files/rakuten_corpus/rakuten_corpus_master/preprocessedfile/type2/forUM/rakuten_preprocessed_302308.txt',\n",
       " './files/rakuten_corpus/rakuten_corpus_master/preprocessedfile/type2/forUM/rakuten_preprocessed_31571.txt',\n",
       " './files/rakuten_corpus/rakuten_corpus_master/preprocessedfile/type2/forUM/rakuten_preprocessed_28738.txt',\n",
       " './files/rakuten_corpus/rakuten_corpus_master/preprocessedfile/type2/forUM/rakuten_preprocessed_134672.txt',\n",
       " './files/rakuten_corpus/rakuten_corpus_master/preprocessedfile/type2/forUM/rakuten_preprocessed_24506.txt',\n",
       " './files/rakuten_corpus/rakuten_corpus_master/preprocessedfile/type2/forUM/rakuten_preprocessed_11384.txt',\n",
       " './files/rakuten_corpus/rakuten_corpus_master/preprocessedfile/type2/forUM/rakuten_preprocessed_14724.txt',\n",
       " './files/rakuten_corpus/rakuten_corpus_master/preprocessedfile/type2/forUM/rakuten_preprocessed_21356.txt',\n",
       " './files/rakuten_corpus/rakuten_corpus_master/preprocessedfile/type2/forUM/rakuten_preprocessed_101930.txt',\n",
       " './files/rakuten_corpus/rakuten_corpus_master/preprocessedfile/type2/forUM/rakuten_preprocessed_68715.txt',\n",
       " './files/rakuten_corpus/rakuten_corpus_master/preprocessedfile/type2/forUM/rakuten_preprocessed_233439.txt',\n",
       " './files/rakuten_corpus/rakuten_corpus_master/preprocessedfile/type2/forUM/rakuten_preprocessed_18413.txt',\n",
       " './files/rakuten_corpus/rakuten_corpus_master/preprocessedfile/type2/forUM/rakuten_preprocessed_203179.txt',\n",
       " './files/rakuten_corpus/rakuten_corpus_master/preprocessedfile/type2/forUM/rakuten_preprocessed_168628.txt',\n",
       " './files/rakuten_corpus/rakuten_corpus_master/preprocessedfile/type2/forUM/rakuten_preprocessed_8452.txt',\n",
       " './files/rakuten_corpus/rakuten_corpus_master/preprocessedfile/type2/forUM/rakuten_preprocessed_270667.txt',\n",
       " './files/rakuten_corpus/rakuten_corpus_master/preprocessedfile/type2/forUM/rakuten_preprocessed_5105.txt']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for path in list_filepath:\n",
    "    list_sepword = Filer.readtxt(path)\n",
    "    list_sepword = [row.split(' ') for row in list_sepword]\n",
    "    list_edgelist = []\n",
    "    for row in list_sepword:\n",
    "        list_tmp = list(itertools.combinations(row,2))\n",
    "        list_edgelist.extend(list_tmp)\n",
    "    list_edgelist = [' '.join(row) for row in list_edgelist]\n",
    "    path_rev = path.replace('./files/rakuten_corpus/rakuten_corpus_master/preprocessedfile/type2/forUM/rakuten_preprocessed', '')\n",
    "    Filer.writetxt(list_edgelist, './files/rakuten_corpus/rakuten_corpus_master/preprocessedfile/type2/forPRTM/rakuten_edgelist' + path_rev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### PRTM用のコーパスを作成する(type2, bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_filepath = glob.glob('./files/rakuten_corpus/rakuten_corpus_master/preprocessedfile/type2/forUM/*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for path in list_filepath:\n",
    "    list_sepword = Filer.readtxt(path)\n",
    "    list_sepword = [row.split(' ') for row in list_sepword]\n",
    "    list_edgelist = []\n",
    "    for row in list_sepword:\n",
    "        list_tmp = []\n",
    "        for i in range(len(row)-1):\n",
    "            list_tmp.append([row[i], row[i+1]])\n",
    "        list_edgelist.extend(list_tmp)\n",
    "    list_edgelist = [' '.join(row) for row in list_edgelist]\n",
    "    path_rev = path.replace('./files/rakuten_corpus/rakuten_corpus_master/preprocessedfile/type2/forUM/rakuten_preprocessed', '')\n",
    "    Filer.writetxt(list_edgelist, './files/rakuten_corpus/rakuten_corpus_master/preprocessedfile/type2/forPRTM_bigram/rakuten_bigram' + path_rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テスト用コーパスを作る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_testfile = Filer.readdump('files/rakuten_corpus/annotation/all_sep_full.dump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_testfile = [[row[2], row[1]] for row in list_testfile]\n",
    "Filer.writedump(list_testfile, './files/rakuten_corpus/rakuten_corpus_master/testfile/list_sepword_label.dump')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 学習用コーパス(type3)を作成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_filename = [\"./files/rakuten_corpus/rakuten_corpus_master/sentencefile/rakuten_sentence_1000.txt\",\n",
    "                 \"./files/rakuten_corpus/rakuten_corpus_master/sentencefile/rakuten_sentence_2000.txt\",\n",
    "                 \"./files/rakuten_corpus/rakuten_corpus_master/sentencefile/rakuten_sentence_3000.txt\",\n",
    "                 \"./files/rakuten_corpus/rakuten_corpus_master/sentencefile/rakuten_sentence_4000.txt\",\n",
    "                 \"./files/rakuten_corpus/rakuten_corpus_master/sentencefile/rakuten_sentence_5000.txt\",\n",
    "                 \"./files/rakuten_corpus/rakuten_corpus_master/sentencefile/rakuten_sentence_6000.txt\",\n",
    "                 \"./files/rakuten_corpus/rakuten_corpus_master/sentencefile/rakuten_sentence_7000.txt\",\n",
    "                 \"./files/rakuten_corpus/rakuten_corpus_master/sentencefile/rakuten_sentence_8000.txt\",\n",
    "                 \"./files/rakuten_corpus/rakuten_corpus_master/sentencefile/rakuten_sentence_9000.txt\",\n",
    "                 \"./files/rakuten_corpus/rakuten_corpus_master/sentencefile/rakuten_sentence_10000.txt\",\n",
    "                 \"./files/rakuten_corpus/rakuten_corpus_master/sentencefile/rakuten_sentence_20000.txt\",\n",
    "                 \"./files/rakuten_corpus/rakuten_corpus_master/sentencefile/rakuten_sentence_30000.txt\",\n",
    "                 \"./files/rakuten_corpus/rakuten_corpus_master/sentencefile/rakuten_sentence_40000.txt\",\n",
    "                 \"./files/rakuten_corpus/rakuten_corpus_master/sentencefile/rakuten_sentence_50000.txt\",\n",
    "                 \"./files/rakuten_corpus/rakuten_corpus_master/sentencefile/rakuten_sentence_60000.txt\",\n",
    "                 \"./files/rakuten_corpus/rakuten_corpus_master/sentencefile/rakuten_sentence_70000.txt\",\n",
    "                 \"./files/rakuten_corpus/rakuten_corpus_master/sentencefile/rakuten_sentence_80000.txt\",\n",
    "                 \"./files/rakuten_corpus/rakuten_corpus_master/sentencefile/rakuten_sentence_90000.txt\",\n",
    "                 \"./files/rakuten_corpus/rakuten_corpus_master/sentencefile/rakuten_sentence_100000.txt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for path in list_filepath:\n",
    "    list_sentence = Filer.readtxt(path)\n",
    "    list_word = [parsing_rev(sentence) for sentence in list_sentence]\n",
    "    list_word = [\" \".join(row) for row in list_word if len(row) >= 2]\n",
    "    Filer.writetxt(list_word, \"files/rakuten_corpus/rakuten_corpus_master/preprocessedfile/type3/forUM/rakuten_preprocessed_%s.txt\" % len(list_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テストコーパス作り(type3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_sentencefile = Filer.readtsv('./files/rakuten_corpus/annotation01_tsukuba_corpus_20140930.tsv')\n",
    "list_anno = Filer.readdump('./files/rakuten_corpus/annotation/all_sep.dump')\n",
    "\n",
    "dict_id_sentence = {int(row[0]): row[5] for row in list_sentencefile if row[0] != ''}\n",
    "list_id_labels = [[row[0], row[1]] for row in list_anno]\n",
    "\n",
    "list_sentence_label = [[dict_id_sentence[row[0]], row[1]] for row in list_id_labels]\n",
    "\n",
    "list_sepword_label = [[parsing_rev(row[0]), row[1]] for row in list_sentence_label]\n",
    "\n",
    "Filer.writedump(list_sepword_label, './files/rakuten_corpus/rakuten_corpus_master/testfile/list_sepword_label_type3.dump')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRTM用のコーパス作り(type3, bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_filepath = glob.glob('./files/rakuten_corpus/rakuten_corpus_master/preprocessedfile/type3/forUM/*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for path in list_filepath:\n",
    "    list_sepword = Filer.readtxt(path)\n",
    "    list_sepword = [row.split(' ') for row in list_sepword]\n",
    "    list_edgelist = []\n",
    "    for row in list_sepword:\n",
    "        list_tmp = []\n",
    "        for i in range(len(row)-1):\n",
    "            list_tmp.append([row[i], row[i+1]])\n",
    "        list_edgelist.extend(list_tmp)\n",
    "    list_edgelist = [' '.join(row) for row in list_edgelist]\n",
    "    path_rev = path.replace('./files/rakuten_corpus/rakuten_corpus_master/preprocessedfile/type3/forUM/rakuten_preprocessed', '')\n",
    "    Filer.writetxt(list_edgelist, './files/rakuten_corpus/rakuten_corpus_master/preprocessedfile/type3/forPRTM_bigram/rakuten_bigram' + path_rev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRTM用のコーパス作り(type3, cor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_filepath = glob.glob('./files/rakuten_corpus/rakuten_corpus_master/preprocessedfile/type3/forUM/*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for path in list_filepath:\n",
    "    list_sepword = Filer.readtxt(path)\n",
    "    list_sepword = [row.split(' ') for row in list_sepword]\n",
    "    list_edgelist = []\n",
    "    for row in list_sepword:\n",
    "        list_tmp = list(itertools.combinations(row,2))\n",
    "        list_edgelist.extend(list_tmp)\n",
    "    list_edgelist = [' '.join(row) for row in list_edgelist]\n",
    "    path_rev = path.replace('./files/rakuten_corpus/rakuten_corpus_master/preprocessedfile/type3/forUM/rakuten_preprocessed', '')\n",
    "    Filer.writetxt(list_edgelist, './files/rakuten_corpus/rakuten_corpus_master/preprocessedfile/type3/forPRTM_cor/rakuten_edgelist' + path_rev)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
