{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\"\"\"\n",
    "corenlpを使って形態素解析を行って学習用のファイルを作成する\n",
    "\"\"\"\n",
    "from stanford_corenlp_pywrapper import CoreNLP\n",
    "from library.filer import Filer\n",
    "import re\n",
    "import collections\n",
    "import itertools\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 形態素解析する600ユーザー分のレビューを取得\n",
    "2. CoreNLPの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_sentence = Filer.readtxt(\"./files/edmunds/\")\n",
    "\n",
    "proc = CoreNLP(configdict={'annotators': 'tokenize,ssplit,pos, lemma'}, corenlp_jars=[\"/home/ikegami/lib/stanford-corenlp-full-2015-04-20/*\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 形態素解析を行って、NN, VB, JJに限定して単語を取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n",
      "error\n"
     ]
    }
   ],
   "source": [
    "pattern1 = r'^NN'\n",
    "pattern2 = r'^VB'\n",
    "pattern3 = r'^JJ'\n",
    "\n",
    "list_sentence_sep=[]\n",
    "list_sentence_rev = []\n",
    "for row in list_sentence[0:600]:\n",
    "    list_user_tmp = []\n",
    "    list_sentence_tmp = []\n",
    "    for sentence in row:\n",
    "        list_sen_tmp = []\n",
    "        try:\n",
    "            dict_tmp = proc.parse_doc(sentence)\n",
    "            for lemma, pos in zip(dict_tmp[u'sentences'][0][u'lemmas'], dict_tmp[u'sentences'][0][u'pos']):\n",
    "                if re.match(pattern1, pos) or re.match(pattern2, pos) or re.match(pattern3, pos):\n",
    "                    list_sen_tmp.append(lemma)\n",
    "            list_user_tmp.append(list_sen_tmp)\n",
    "            list_sentence_tmp.append(sentence)\n",
    "        except UnicodeError:\n",
    "            print \"error\"\n",
    "    list_sentence_sep.append(list_user_tmp)\n",
    "    list_sentence_rev.append(list_sentence_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 単語を除去する前の語彙数と語数をチェック"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27385\n",
      "3975\n"
     ]
    }
   ],
   "source": [
    "list_words = [word for row1 in list_sentence_sep for row2 in row1 for word in row2]\n",
    "print len(list_words)\n",
    "print len(set(list_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 出現頻度300以上、2以下の単語は除去語として、登録\n",
    "2. 除去語を形態素解析した結果から削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_tmp = [sen for row1 in list_sentence_sep for row2 in row1 for sen in row2]\n",
    "list_count = sorted(collections.Counter(list_tmp).items(), key=lambda x: x[1], reverse=True)\n",
    "list_remove = [row[0] for row in list_count if row[1] >= 300 or row[1] <= 2]\n",
    "\n",
    "list_sentence_sep_rev = []\n",
    "for row1 in list_sentence_sep:\n",
    "    list_sentence_tmp = []\n",
    "    for row2 in row1:\n",
    "        list_tmp = []\n",
    "        for word in row2:\n",
    "            if word not in list_remove:\n",
    "                list_tmp.append(word.encode(\"utf-8\"))\n",
    "        list_sentence_tmp.append(list_tmp)\n",
    "    list_sentence_sep_rev.append(list_sentence_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. userID, sentenceID, sentence, sep_wordsという形式のリストを作成\n",
    "2. tsv形式でファイルを保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_all = []\n",
    "counter_user = 0\n",
    "counter_sen = 0\n",
    "for row_sen, row_sep in zip(list_sentence_rev, list_sentence_sep_rev):\n",
    "    counter_sen = 0\n",
    "    for sen, sep in zip(row_sen, row_sep):\n",
    "        list_tmp = [counter_user, counter_sen, sen]\n",
    "        list_tmp.extend(sep)\n",
    "        counter_sen += 1\n",
    "        list_all.append(list_tmp)\n",
    "    counter_user += 1\n",
    "    \n",
    "Filer.writetsv(list_all, \"./files/edmunds_car/list_sentence_sep.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. UserID, sentenceID, sentence, sepを記録したtsvファイルを読み込む\n",
    "2. sepの部分のみ抜き出す\n",
    "3. itertoolsを使ってエッジリストを作成\n",
    "4. csvファイルとしてエッジリストを保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_all = Filer.readtsv(\"./files/edmunds_car/list_sentence_sep.tsv\")\n",
    "\n",
    "list_all = [row[3:] for row in list_all]\n",
    "\n",
    "list_edgelist = []\n",
    "for row in list_all:\n",
    "    list_tmp = list(itertools.combinations(row,2))\n",
    "    list_edgelist.extend(list_tmp)\n",
    "    \n",
    "Filer.writecsv(list_edgelist, \"./files/edmunds_car/list_edgelist_edmunds_co.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "1. UserID, sentenceID, sentence, sepを記録したtsvファイルを読み込む\n",
    "2. sepの部分のみ抜き出す\n",
    "3. UM用の学習ファイルとして、txtファイルとして保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_all = Filer.readtsv(\"./files/edmunds/list_sentence_sep.tsv\")\n",
    "\n",
    "list_all = [\" \".join(row[3:]) for row in list_all]\n",
    "\n",
    "Filer.writetxt(list_all, \"./files/edmunds/edmunds_corpus_for_UM.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1. UserID, sentenceID, sentence, sepを記録したtsvファイルを読み込む\n",
    "2. UserID, sentenceID, sentenceの部分のみ抜き出す\n",
    "3. sentencesIDを付け直す\n",
    "4. tsvファイルに保存する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_all = Filer.readtsv(\"./files/edmunds/list_sentence_sep.tsv\")\n",
    "\n",
    "list_all = [row[:3] for row in list_all]\n",
    "\n",
    "list_all_rev = []\n",
    "for i, row in enumerate(list_all):\n",
    "    list_all_rev.append([row[0], i, row[2]])\n",
    "    \n",
    "Filer.writetsv(list_all_rev, \"./files/edmunds/edmunds_annotation.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1. UserID, sentenceID, sentence, sepを記録したtsvファイルを読み込む\n",
    "2. sentencesIDを付け直す\n",
    "3. tsvファイルに保存する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_all = Filer.readtsv(\"./files/edmunds/list_sentence_sep.tsv\")\n",
    "\n",
    "for i, row in enumerate(list_all):\n",
    "    list_all[i][1] = i\n",
    "    \n",
    "Filer.writetsv(list_all, \"./files/edmunds/list_sentence_sep_rev.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = Filer.readdump(\"./files/rakuten_corpus/annotation/all_sep.dump\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 評価用のダンプファイルを作成する\n",
    "* [sentenceID, label, [sepwords]]\n",
    "* その際に、空の配列は消すようにする\n",
    "1. list_sentence_sep.tsv, list_sentence_annotation.tsvを読み込み\n",
    "2. 配列の作成\n",
    "3. 形態素の数が0のものを除く\n",
    "4. それぞれのラベルが何個ずつあるのかカウントする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 125), (1, 149), (2, 102), (3, 106), (4, 352), (5, 168), (6, 71), (7, 75)]\n"
     ]
    }
   ],
   "source": [
    "# コーパスの読み込み\n",
    "list_sentence_sep = Filer.readtsv(\"./files/edmunds/list_sentence_sep.tsv\")\n",
    "list_sentence_annotation = Filer.readtsv(\"./files/edmunds/list_sentence_annotation.tsv\")\n",
    "\n",
    "# key: id, value: list_sepの辞書を作成\n",
    "dict_id_sep = {int(row[1]): row[3:] for row in list_sentence_sep}\n",
    "# [sentenceID, label, [sepwords]]の配列を作成、ただし、形態素数が0の場合は除く\n",
    "list_id_label_sep = [[int(row[1]), int(row[3]), dict_id_sep[int(row[1])]] for row in list_sentence_annotation if len(dict_id_sep[int(row[1])]) != 0]\n",
    "list_label = [row[1] for row in list_id_label_sep]\n",
    "print sorted(collections.Counter(list_label).items(), key=lambda x:x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Filer.writedump(list_id_label_sep, \"./files/edmunds/list_id_label_sep.dump\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA用の学習データを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_all = Filer.readtsv(\"./files/edmunds/list_sentence_sep.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dict_id_sep = {}\n",
    "for row in list_all:\n",
    "    if int(row[0]) in dict_id_sep:\n",
    "        dict_id_sep[int(row[0])].extend(row[3:])\n",
    "    else:\n",
    "        dict_id_sep[int(row[0])] = row[3:]\n",
    "        \n",
    "list_sep = [row for row in sorted(dict_id_sep.items(), key=lambda x: x[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Filer.writecsv(list_sep, \"./files/edmunds/edmunds_corpus_for_LDA.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 学習コーパス（type1）を作る\n",
    "* 頻度２以下の単語を削除\n",
    "* 単語数が１以下の文を削除\n",
    "* ただしユーザ数が100000以下のファイルのみ（計算が終わらないから）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:CoreNLP_PyWrapper:Starting java subprocess, and waiting for signal it's ready, with command: exec java -Xmx4g -XX:ParallelGCThreads=1 -cp '/home/ikegami/ikegami/lib/python2.7/site-packages/stanford_corenlp_pywrapper/lib/*:/home/ikegami/lib/stanford-corenlp-full-2015-04-20/*'      corenlp.SocketServer --outpipe /tmp/corenlp_pywrap_pipe_pypid=29972_time=1466058997.68  --configdict '{\"annotators\": \"tokenize,ssplit,pos, lemma\"}'\n",
      "INFO:CoreNLP_PyWrapper:Successful ping. The server has started.\n",
      "INFO:CoreNLP_PyWrapper:Subprocess is ready.\n"
     ]
    }
   ],
   "source": [
    "proc = CoreNLP(configdict={'annotators': 'tokenize,ssplit,pos, lemma'}, corenlp_jars=[\"/home/ikegami/lib/stanford-corenlp-full-2015-04-20/*\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_filepath = glob.glob('./files/edmunds/edmunds_corpus_master/sentencefile/edmunds_sentence_*.txt')\n",
    "list_testfile = Filer.readtsv('files/edmunds/list_sentence_sep.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n"
     ]
    }
   ],
   "source": [
    "for path in list_filepath:\n",
    "    list_sentence = Filer.readtxt(path, LF='\\n')\n",
    "    pattern1 = r'^NN'\n",
    "    pattern2 = r'^VB'\n",
    "    pattern3 = r'^JJ'\n",
    "    for row in list_testfile:\n",
    "        list_sentence.append(row[2])\n",
    "\n",
    "    list_sepword = []\n",
    "    list_remove = []\n",
    "    dict_word_freq = {}\n",
    "    for sentence in list_sentence:\n",
    "        list_sepword_tmp = []\n",
    "        try:\n",
    "            dict_tmp = proc.parse_doc(sentence)\n",
    "            for lemma, pos in zip(dict_tmp[u'sentences'][0][u'lemmas'], dict_tmp[u'sentences'][0][u'pos']):\n",
    "                if re.match(pattern1, pos) or re.match(pattern2, pos) or re.match(pattern3, pos):\n",
    "                    list_sepword_tmp.append(lemma.encode('utf-8'))\n",
    "                    if lemma.encode('utf-8') in dict_word_freq:\n",
    "                        dict_word_freq[lemma.encode('utf-8')] += 1\n",
    "                    else:\n",
    "                        dict_word_freq[lemma.encode('utf-8')] = 1\n",
    "            list_sepword.append(list_sepword_tmp)\n",
    "        except UnicodeError:\n",
    "            print \"error\"\n",
    "    \n",
    "    for word, value in dict_word_freq.items():\n",
    "        if value <= 2:\n",
    "            list_remove.append(word)\n",
    "            \n",
    "    list_sepword = [[word for word in row if word not in list_remove] for row in list_sepword]\n",
    "    list_sepword = [' '.join(row) for row in list_sepword if len(row) >= 2]\n",
    "            \n",
    "    Filer.writetxt(list_sepword, 'files/edmunds/edmunds_corpus_master/preprocessedfile/type1/forUM/edmunds_preprocessed_%s.txt' % len(list_sepword))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習コーパス（type2）を作る\n",
    "* 頻度２以下の単語を削除\n",
    "* 出現頻度 6% 以上の単語の削除\n",
    "* 単語数が１以下の文を削除\n",
    "* ただしユーザ数が100000以下のファイルのみ（計算が終わらないから）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:CoreNLP_PyWrapper:Starting java subprocess, and waiting for signal it's ready, with command: exec java -Xmx4g -XX:ParallelGCThreads=1 -cp '/home/ikegami/ikegami/lib/python2.7/site-packages/stanford_corenlp_pywrapper/lib/*:/home/ikegami/lib/stanford-corenlp-full-2015-04-20/*'      corenlp.SocketServer --outpipe /tmp/corenlp_pywrap_pipe_pypid=29972_time=1466066544.88  --configdict '{\"annotators\": \"tokenize,ssplit,pos, lemma\"}'\n",
      "INFO:CoreNLP_PyWrapper:Successful ping. The server has started.\n",
      "INFO:CoreNLP_PyWrapper:Subprocess is ready.\n"
     ]
    }
   ],
   "source": [
    "proc = CoreNLP(configdict={'annotators': 'tokenize,ssplit,pos, lemma'}, corenlp_jars=[\"/home/ikegami/lib/stanford-corenlp-full-2015-04-20/*\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_filepath = glob.glob('./files/edmunds/edmunds_corpus_master/sentencefile/edmunds_sentence_*.txt')\n",
    "list_testfile = Filer.readtsv('files/edmunds/list_sentence_sep.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n",
      "error\n",
      "error"
     ]
    }
   ],
   "source": [
    "for path in list_filepath:\n",
    "    list_sentence = Filer.readtxt(path, LF='\\n')\n",
    "    pattern1 = r'^NN'\n",
    "    pattern2 = r'^VB'\n",
    "    pattern3 = r'^JJ'\n",
    "    for row in list_testfile:\n",
    "        list_sentence.append(row[2])\n",
    "\n",
    "    list_sepword = []\n",
    "    list_remove = []\n",
    "    # 出現回数\n",
    "    dict_word_freq = {}\n",
    "    # 出現頻度\n",
    "    dict_document_word = {}\n",
    "    for sentence in list_sentence:\n",
    "        list_sepword_tmp = []\n",
    "        try:\n",
    "            dict_tmp = proc.parse_doc(sentence)\n",
    "            for lemma, pos in zip(dict_tmp[u'sentences'][0][u'lemmas'], dict_tmp[u'sentences'][0][u'pos']):\n",
    "                if re.match(pattern1, pos) or re.match(pattern2, pos) or re.match(pattern3, pos):\n",
    "                    list_sepword_tmp.append(lemma.encode('utf-8'))\n",
    "                    if lemma.encode('utf-8') in dict_word_freq:\n",
    "                        dict_word_freq[lemma.encode('utf-8')] += 1\n",
    "                    else:\n",
    "                        dict_word_freq[lemma.encode('utf-8')] = 1\n",
    "            # 頻度の記録\n",
    "            for word in set(list_sepword_tmp):\n",
    "                if word in dict_document_word:\n",
    "                    dict_document_word[word] += 1\n",
    "                else:\n",
    "                    dict_document_word[word] = 1\n",
    "\n",
    "            list_sepword.append(list_sepword_tmp)\n",
    "        except UnicodeError:\n",
    "            print \"error\"\n",
    "    dict_document_word = {word: float(freq)/len(list_sepword) for word, freq in dict_document_word.items()}\n",
    "    \n",
    "    for word, value in dict_word_freq.items():\n",
    "        if value <= 2:\n",
    "            list_remove.append(word)\n",
    "            \n",
    "    for word, value in dict_document_word.items():\n",
    "        if value >= 0.06:\n",
    "            list_remove.append(word)\n",
    "            \n",
    "    list_sepword = [[word for word in row if word not in list_remove] for row in list_sepword]\n",
    "    list_sepword = [' '.join(row) for row in list_sepword if len(row) >= 2]\n",
    "\n",
    "    Filer.writetxt(list_sepword, 'files/edmunds/edmunds_corpus_master/preprocessedfile/type2/forUM/edmunds_preprocessed_%s.txt' % len(list_sepword))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRTM用の学習ファイルを作る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_filepath = glob.glob('./files/edmunds/edmunds_corpus_master/preprocessedfile/type2/forUM/*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./files/edmunds/edmunds_corpus_master/preprocessedfile/type2/forUM/edmunds_preprocessed_189400.txt',\n",
       " './files/edmunds/edmunds_corpus_master/preprocessedfile/type2/forUM/edmunds_preprocessed_127532.txt',\n",
       " './files/edmunds/edmunds_corpus_master/preprocessedfile/type2/forUM/edmunds_preprocessed_9806.txt',\n",
       " './files/edmunds/edmunds_corpus_master/preprocessedfile/type2/forUM/edmunds_preprocessed_22131.txt',\n",
       " './files/edmunds/edmunds_corpus_master/preprocessedfile/type2/forUM/edmunds_preprocessed_28292.txt',\n",
       " './files/edmunds/edmunds_corpus_master/preprocessedfile/type2/forUM/edmunds_preprocessed_59419.txt',\n",
       " './files/edmunds/edmunds_corpus_master/preprocessedfile/type2/forUM/edmunds_preprocessed_40647.txt',\n",
       " './files/edmunds/edmunds_corpus_master/preprocessedfile/type2/forUM/edmunds_preprocessed_47182.txt',\n",
       " './files/edmunds/edmunds_corpus_master/preprocessedfile/type2/forUM/edmunds_preprocessed_65384.txt',\n",
       " './files/edmunds/edmunds_corpus_master/preprocessedfile/type2/forUM/edmunds_preprocessed_251646.txt',\n",
       " './files/edmunds/edmunds_corpus_master/preprocessedfile/type2/forUM/edmunds_preprocessed_34541.txt',\n",
       " './files/edmunds/edmunds_corpus_master/preprocessedfile/type2/forUM/edmunds_preprocessed_53061.txt',\n",
       " './files/edmunds/edmunds_corpus_master/preprocessedfile/type2/forUM/edmunds_preprocessed_15728.txt']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for path in list_filepath:\n",
    "    list_sepword = Filer.readtxt(path)\n",
    "    list_sepword = [row.split(' ') for row in list_sepword]\n",
    "    list_edgelist = []\n",
    "    for row in list_sepword:\n",
    "        list_tmp = list(itertools.combinations(row,2))\n",
    "        list_edgelist.extend(list_tmp)\n",
    "    list_edgelist = [' '.join(row) for row in list_edgelist]\n",
    "    path_rev = path.replace('./files/edmunds/edmunds_corpus_master/preprocessedfile/type2/forUM/edmunds_preprocessed', '')\n",
    "    Filer.writetxt(list_edgelist, './files/edmunds/edmunds_corpus_master/preprocessedfile/type2/forPRTM/edmunds_edgelist' + path_rev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### PRTM用のコーパス(type2, bigram)を作る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_filepath = glob.glob('./files/edmunds/edmunds_corpus_master/preprocessedfile/type2/forUM/*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./files/edmunds/edmunds_corpus_master/preprocessedfile/type2/forUM/edmunds_preprocessed_189400.txt',\n",
       " './files/edmunds/edmunds_corpus_master/preprocessedfile/type2/forUM/edmunds_preprocessed_127532.txt',\n",
       " './files/edmunds/edmunds_corpus_master/preprocessedfile/type2/forUM/edmunds_preprocessed_9806.txt',\n",
       " './files/edmunds/edmunds_corpus_master/preprocessedfile/type2/forUM/edmunds_preprocessed_22131.txt',\n",
       " './files/edmunds/edmunds_corpus_master/preprocessedfile/type2/forUM/edmunds_preprocessed_28292.txt',\n",
       " './files/edmunds/edmunds_corpus_master/preprocessedfile/type2/forUM/edmunds_preprocessed_59419.txt',\n",
       " './files/edmunds/edmunds_corpus_master/preprocessedfile/type2/forUM/edmunds_preprocessed_40647.txt',\n",
       " './files/edmunds/edmunds_corpus_master/preprocessedfile/type2/forUM/edmunds_preprocessed_47182.txt',\n",
       " './files/edmunds/edmunds_corpus_master/preprocessedfile/type2/forUM/edmunds_preprocessed_65384.txt',\n",
       " './files/edmunds/edmunds_corpus_master/preprocessedfile/type2/forUM/edmunds_preprocessed_251646.txt',\n",
       " './files/edmunds/edmunds_corpus_master/preprocessedfile/type2/forUM/edmunds_preprocessed_34541.txt',\n",
       " './files/edmunds/edmunds_corpus_master/preprocessedfile/type2/forUM/edmunds_preprocessed_53061.txt',\n",
       " './files/edmunds/edmunds_corpus_master/preprocessedfile/type2/forUM/edmunds_preprocessed_15728.txt']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for path in list_filepath:\n",
    "    list_sepword = Filer.readtxt(path)\n",
    "    list_sepword = [row.split(' ') for row in list_sepword]\n",
    "    list_edgelist = []\n",
    "    for row in list_sepword:\n",
    "        list_tmp = []\n",
    "        for i in range(len(row)-1):\n",
    "            list_tmp.append([row[i], row[i+1]])\n",
    "        list_edgelist.extend(list_tmp)\n",
    "    list_edgelist = [' '.join(row) for row in list_edgelist]\n",
    "    path_rev = path.replace('./files/edmunds/edmunds_corpus_master/preprocessedfile/type2/forUM/edmunds_preprocessed', '')\n",
    "    Filer.writetxt(list_edgelist, './files/edmunds/edmunds_corpus_master/preprocessedfile/type2/forPRTM_bigram/edmunds_preprocessed' + path_rev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習コーパス(type3)を作る\n",
    "* 形態素解析\n",
    "* 機能語の除去をしない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_filepath = glob.glob('./files/edmunds/edmunds_corpus_master/sentencefile/edmunds_sentence_*.txt')\n",
    "list_testfile = Filer.readtsv('files/edmunds/list_sentence_sep.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n"
     ]
    }
   ],
   "source": [
    "for path in list_filepath:\n",
    "    list_sentence = Filer.readtxt(path, LF='\\n')\n",
    "    for row in list_testfile:\n",
    "        list_sentence.append(row[2])\n",
    "\n",
    "    list_sepword = []\n",
    "    dict_word_freq = {}\n",
    "    for sentence in list_sentence:\n",
    "        list_sepword_tmp = []\n",
    "        try:\n",
    "            dict_tmp = proc.parse_doc(sentence)\n",
    "            for lemma, pos in zip(dict_tmp[u'sentences'][0][u'lemmas'], dict_tmp[u'sentences'][0][u'pos']):\n",
    "                if lemma.encode('utf-8') != '.' and lemma.encode('utf-8') != ',':\n",
    "                    list_sepword_tmp.append(lemma.encode('utf-8'))\n",
    "            list_sepword.append(list_sepword_tmp)\n",
    "        except UnicodeError:\n",
    "            print \"error\"\n",
    "\n",
    "    list_sepword = [' '.join(row) for row in list_sepword if len(row) >= 2]\n",
    "            \n",
    "    Filer.writetxt(list_sepword, 'files/edmunds/edmunds_corpus_master/preprocessedfile/type3/forUM/edmunds_preprocessed_%s.txt' % len(list_sepword))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRTM用のコーパス(type3, bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_filepath = glob.glob('./files/edmunds/edmunds_corpus_master/preprocessedfile/type3/forUM/*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for path in list_filepath:\n",
    "    list_sepword = Filer.readtxt(path)\n",
    "    list_sepword = [row.split(' ') for row in list_sepword]\n",
    "    list_edgelist = []\n",
    "    for row in list_sepword:\n",
    "        list_tmp = []\n",
    "        for i in range(len(row)-1):\n",
    "            list_tmp.append([row[i], row[i+1]])\n",
    "        list_edgelist.extend(list_tmp)\n",
    "    list_edgelist = [' '.join(row) for row in list_edgelist]\n",
    "    path_rev = path.replace('./files/edmunds/edmunds_corpus_master/preprocessedfile/type3/forUM/edmunds_preprocessed', '')\n",
    "    Filer.writetxt(list_edgelist, './files/edmunds/edmunds_corpus_master/preprocessedfile/type3/forPRTM_bigram/edmunds_bigram' + path_rev)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
